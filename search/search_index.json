{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction Github Link Github Link Overview Titan Robot is a ROS 2\u2013based mobile robot platform designed for learning, experimentation, and real-world robotics development. Built with cost efficiency and modularity in mind, Titan Robot enables students, researchers, and developers to explore modern robotic concepts such as autonomous navigation, mapping, localization, and robot software architecture using ROS 2. The platform combines reliable hardware with a flexible software stack, making it suitable for education, prototyping, and research applications. Titan Robot emphasizes hands-on learning by exposing users to both high-level autonomy and low-level motor control. Design Goals Titan Robot was designed with the following goals: Affordability \u2013 Accessible hardware without compromising core robotics capabilities Modularity \u2013 Easy to extend with additional sensors and peripherals ROS 2 Native \u2013 Built entirely around ROS 2 concepts and workflows Educational Focus \u2013 Clear architecture suitable for learning and teaching Real-World Readiness \u2013 Designed to operate reliably outside simulation Key Capabilities Titan Robot supports a wide range of robotics functionalities out of the box: Differential-drive mobile base Real-time motor control with encoder feedback 2D LIDAR-based mapping and obstacle detection Autonomous navigation using ROS 2 Navigation Stack Teleoperation via keyboard or joystick Visualization and debugging using RViz2 Custom firmware and software development Key Metrics for Titan Robot Category Parameter Value Drive / Base Drive type Differential drive mobile base Drive / Base Motors 2\u00d7 DC gear motors Odometry Encoders Quadrature wheel encoders Compute Main computer Raspberry Pi 4B Compute RAM 4 GB and 8 GB OS Operating system Ubuntu 22.04 LTS Control Low-level controller Custom Momentum Robotics Compute board ESP32 S3 based Control Motor control PWM + PID velocity control Control Compute \u2194 MCU interface Serial interface Sensors LiDAR 2D LiDAR, 360\u00b0 scanning range Sensors Camera V1: No camera V2: Raspberry Pi Camera V3: Intel RealSense depth camera Power Battery voltage 11.1 V Power Battery type FPF battery, 4400 mAh Software Navigation stack ROS 2 Nav2 Interface Visualization RViz2 supported Created By Momentum Robotics is an Indian Robotics Startup. Next Steps To begin working with Titan Robot, proceed to: Getting Started \u2013 System requirements and setup Hardware Overview \u2013 Mechanical and electronic design Software Architecture \u2013 ROS 2 node and communication structure Let\u2019s get started \ud83d\ude80","title":"Home"},{"location":"#overview","text":"Titan Robot is a ROS 2\u2013based mobile robot platform designed for learning, experimentation, and real-world robotics development. Built with cost efficiency and modularity in mind, Titan Robot enables students, researchers, and developers to explore modern robotic concepts such as autonomous navigation, mapping, localization, and robot software architecture using ROS 2. The platform combines reliable hardware with a flexible software stack, making it suitable for education, prototyping, and research applications. Titan Robot emphasizes hands-on learning by exposing users to both high-level autonomy and low-level motor control.","title":"Overview"},{"location":"#design-goals","text":"Titan Robot was designed with the following goals: Affordability \u2013 Accessible hardware without compromising core robotics capabilities Modularity \u2013 Easy to extend with additional sensors and peripherals ROS 2 Native \u2013 Built entirely around ROS 2 concepts and workflows Educational Focus \u2013 Clear architecture suitable for learning and teaching Real-World Readiness \u2013 Designed to operate reliably outside simulation","title":"Design Goals"},{"location":"#key-capabilities","text":"Titan Robot supports a wide range of robotics functionalities out of the box: Differential-drive mobile base Real-time motor control with encoder feedback 2D LIDAR-based mapping and obstacle detection Autonomous navigation using ROS 2 Navigation Stack Teleoperation via keyboard or joystick Visualization and debugging using RViz2 Custom firmware and software development","title":"Key Capabilities"},{"location":"#key-metrics-for-titan-robot","text":"Category Parameter Value Drive / Base Drive type Differential drive mobile base Drive / Base Motors 2\u00d7 DC gear motors Odometry Encoders Quadrature wheel encoders Compute Main computer Raspberry Pi 4B Compute RAM 4 GB and 8 GB OS Operating system Ubuntu 22.04 LTS Control Low-level controller Custom Momentum Robotics Compute board ESP32 S3 based Control Motor control PWM + PID velocity control Control Compute \u2194 MCU interface Serial interface Sensors LiDAR 2D LiDAR, 360\u00b0 scanning range Sensors Camera V1: No camera V2: Raspberry Pi Camera V3: Intel RealSense depth camera Power Battery voltage 11.1 V Power Battery type FPF battery, 4400 mAh Software Navigation stack ROS 2 Nav2 Interface Visualization RViz2 supported","title":"Key Metrics for Titan Robot"},{"location":"#created-by","text":"Momentum Robotics is an Indian Robotics Startup.","title":"Created By"},{"location":"#next-steps","text":"To begin working with Titan Robot, proceed to: Getting Started \u2013 System requirements and setup Hardware Overview \u2013 Mechanical and electronic design Software Architecture \u2013 ROS 2 node and communication structure Let\u2019s get started \ud83d\ude80","title":"Next Steps"},{"location":"about/","text":"About Titan Robot Overview Titan Robot is a ROS 2\u2013based mobile robot platform designed to provide hands-on experience with modern robotics concepts. It combines real hardware, real sensors, and real software workflows to help users understand how autonomous robots are built, programmed, and deployed. The platform emphasizes: - Practical learning over abstraction - Clear system architecture - Modular hardware and software design - Real-world robotics challenges Titan Robot is suitable for education, research, and independent exploration of mobile robotics. Educational Philosophy Titan Robot is designed to bridge the gap between theory and practice. Users are encouraged to experiment, modify, break, and rebuild systems to gain a deeper understanding of robotics. The documentation follows a progressive learning approach: - Start with basic bringup and teleoperation - Move to mapping, localization, and navigation - Advance toward perception, autonomy, and customization Learning Outcomes By working with Titan Robot, users can learn: ROS 2 fundamentals and architecture Differential-drive kinematics Encoder-based odometry Sensor integration and TF management Mapping and localization Autonomous navigation with Nav2 Debugging real robotic systems Software\u2013hardware integration Resources Core Software Resources ROS 2 Humble Documentation Navigation2 Documentation ROS 2 Tutorials RViz2 User Guide Gazebo User Guide Slam Toolbox Docs These resources provide deeper insights into the tools and frameworks used by Titan Robot. Hardware & Sensor Resources 2D LIDAR documentation ESP32 microcontroller documentation Raspberry Pi documentation Camera and depth sensor documentation Refer to manufacturer documentation for electrical, mechanical, and performance details. Recommended Tools RViz2 for visualization rqt for debugging Git for version control VS Code for development PlotJuggler or rqt_plot for data analysis Lab Exercises The following lab exercises are designed for structured learning and classroom use. Lab 1: Robot Bringup and Verification Launch the robot software stack Verify sensor topics and TF tree Test keyboard teleoperation Lab 2: Odometry and Motion Analysis Observe encoder-based odometry Measure drift and error Tune motion parameters Lab 3: Mapping an Indoor Environment Create a 2D occupancy grid map Analyze map quality Save and reuse maps Lab 4: Localization Accuracy Initialize robot pose Test localization convergence Observe behavior under motion Lab 5: Autonomous Navigation Send navigation goals Analyze costmaps Test obstacle avoidance Mini Projects Mini projects encourage creativity and deeper exploration. Project 1: Navigation Performance Evaluation Measure navigation accuracy Compare different parameter settings Analyze success rate and failures Project 2: Obstacle-Aware Navigation Modify costmap parameters Test robot behavior in cluttered environments Project 3: Vision-Based Object Detection (V2) Integrate object detection pipeline Visualize detected objects in RViz2 Project 4: Depth-Aware Perception (V3) Use depth data for obstacle understanding Experiment with 3D perception concepts Project 5: Custom Behavior Development Implement custom behaviors using ROS 2 nodes Combine navigation and perception Design autonomous routines Intended Use Cases Titan Robot can be used for: Robotics coursework and labs Research prototyping Autonomous navigation experiments ROS 2 training and workshops Independent robotics projects Contributions and Growth Titan Robot is designed to evolve through continuous improvement. Users are encouraged to: - Experiment with new features - Improve documentation - Add new lab exercises - Extend hardware and software capabilities Final Notes Robotics is best learned by doing. Titan Robot provides a practical platform to explore real-world robotics challenges while building strong foundational skills. This documentation serves as a guide\u2014but true learning happens through experimentation. Happy building and exploring \ud83d\ude80","title":"About"},{"location":"about/#about-titan-robot","text":"","title":"About Titan Robot"},{"location":"about/#overview","text":"Titan Robot is a ROS 2\u2013based mobile robot platform designed to provide hands-on experience with modern robotics concepts. It combines real hardware, real sensors, and real software workflows to help users understand how autonomous robots are built, programmed, and deployed. The platform emphasizes: - Practical learning over abstraction - Clear system architecture - Modular hardware and software design - Real-world robotics challenges Titan Robot is suitable for education, research, and independent exploration of mobile robotics.","title":"Overview"},{"location":"about/#educational-philosophy","text":"Titan Robot is designed to bridge the gap between theory and practice. Users are encouraged to experiment, modify, break, and rebuild systems to gain a deeper understanding of robotics. The documentation follows a progressive learning approach: - Start with basic bringup and teleoperation - Move to mapping, localization, and navigation - Advance toward perception, autonomy, and customization","title":"Educational Philosophy"},{"location":"about/#learning-outcomes","text":"By working with Titan Robot, users can learn: ROS 2 fundamentals and architecture Differential-drive kinematics Encoder-based odometry Sensor integration and TF management Mapping and localization Autonomous navigation with Nav2 Debugging real robotic systems Software\u2013hardware integration","title":"Learning Outcomes"},{"location":"about/#resources","text":"","title":"Resources"},{"location":"about/#core-software-resources","text":"ROS 2 Humble Documentation Navigation2 Documentation ROS 2 Tutorials RViz2 User Guide Gazebo User Guide Slam Toolbox Docs These resources provide deeper insights into the tools and frameworks used by Titan Robot.","title":"Core Software Resources"},{"location":"about/#hardware-sensor-resources","text":"2D LIDAR documentation ESP32 microcontroller documentation Raspberry Pi documentation Camera and depth sensor documentation Refer to manufacturer documentation for electrical, mechanical, and performance details.","title":"Hardware &amp; Sensor Resources"},{"location":"about/#recommended-tools","text":"RViz2 for visualization rqt for debugging Git for version control VS Code for development PlotJuggler or rqt_plot for data analysis","title":"Recommended Tools"},{"location":"about/#lab-exercises","text":"The following lab exercises are designed for structured learning and classroom use.","title":"Lab Exercises"},{"location":"about/#lab-1-robot-bringup-and-verification","text":"Launch the robot software stack Verify sensor topics and TF tree Test keyboard teleoperation","title":"Lab 1: Robot Bringup and Verification"},{"location":"about/#lab-2-odometry-and-motion-analysis","text":"Observe encoder-based odometry Measure drift and error Tune motion parameters","title":"Lab 2: Odometry and Motion Analysis"},{"location":"about/#lab-3-mapping-an-indoor-environment","text":"Create a 2D occupancy grid map Analyze map quality Save and reuse maps","title":"Lab 3: Mapping an Indoor Environment"},{"location":"about/#lab-4-localization-accuracy","text":"Initialize robot pose Test localization convergence Observe behavior under motion","title":"Lab 4: Localization Accuracy"},{"location":"about/#lab-5-autonomous-navigation","text":"Send navigation goals Analyze costmaps Test obstacle avoidance","title":"Lab 5: Autonomous Navigation"},{"location":"about/#mini-projects","text":"Mini projects encourage creativity and deeper exploration.","title":"Mini Projects"},{"location":"about/#project-1-navigation-performance-evaluation","text":"Measure navigation accuracy Compare different parameter settings Analyze success rate and failures","title":"Project 1: Navigation Performance Evaluation"},{"location":"about/#project-2-obstacle-aware-navigation","text":"Modify costmap parameters Test robot behavior in cluttered environments","title":"Project 2: Obstacle-Aware Navigation"},{"location":"about/#project-3-vision-based-object-detection-v2","text":"Integrate object detection pipeline Visualize detected objects in RViz2","title":"Project 3: Vision-Based Object Detection (V2)"},{"location":"about/#project-4-depth-aware-perception-v3","text":"Use depth data for obstacle understanding Experiment with 3D perception concepts","title":"Project 4: Depth-Aware Perception (V3)"},{"location":"about/#project-5-custom-behavior-development","text":"Implement custom behaviors using ROS 2 nodes Combine navigation and perception Design autonomous routines","title":"Project 5: Custom Behavior Development"},{"location":"about/#intended-use-cases","text":"Titan Robot can be used for: Robotics coursework and labs Research prototyping Autonomous navigation experiments ROS 2 training and workshops Independent robotics projects","title":"Intended Use Cases"},{"location":"about/#contributions-and-growth","text":"Titan Robot is designed to evolve through continuous improvement. Users are encouraged to: - Experiment with new features - Improve documentation - Add new lab exercises - Extend hardware and software capabilities","title":"Contributions and Growth"},{"location":"about/#final-notes","text":"Robotics is best learned by doing. Titan Robot provides a practical platform to explore real-world robotics challenges while building strong foundational skills. This documentation serves as a guide\u2014but true learning happens through experimentation. Happy building and exploring \ud83d\ude80","title":"Final Notes"},{"location":"bringup/","text":"Robot Bringup \ud83e\udd16 Overview This section explains how to build, launch, and verify the complete software stack of Titan Robot . Robot bringup refers to starting all essential components required for the robot to operate correctly, including sensors, motor control, state publishing, and visualization. Prerequisites Before proceeding, ensure that: Ubuntu 22.04 is installed ROS 2 Humble is installed and sourced The robot hardware is fully assembled Battery is charged and power system is functional ESP32 firmware is already flashed All sensors are properly connected Update system packages and install essential tools sudo apt update && sudo apt upgrade -y sudo apt install -y curl wget htop net-tools openssh-server rsync tmux python3 python3-pip python3-rosdep gazebo git python3-colcon-common-extensions terminator sudo apt install ros-humble-rviz2 ros-humble-slam-toolbox ros-humble-turtlebot3-gazebo ros-humble-joint-state-publisher-gui ros-humble-gazebo-ros-pkgs One time setup echo 'export TURTLEBOT3_MODEL=burger' >> ~/.bashrc echo 'source /usr/share/gazebo/setup.sh' >> ~/.bashrc echo 'export GAZEBO_MODEL_PATH=~/titan_ws/src/titan_simulation/turtlebot3_gazebo/models:$GAZEBO_MODEL_PATH' >> ~/.bashrc echo 'source ~/titan_ws/install/setup.bash' >> ~/.bashrc Workspace Setup Create and initialize a ROS 2 workspace: mkdir -p ~/titan_ws cd ~/titan_ws Clone the Titan Robot repository: git clone https://github.com/MRS111-OS/titan_robot.git mv titan_robot/ src Install dependencies sudo rosdep init rosdep update rosdep install --from-paths src --ignore-src -y Build the workspace: cd ~/titan_ws colcon build --symlink-install --parallel-workers 4 Source the workspace: source install/setup.bash It is recommended to add the source command to your .bashrc for convenience. Bringup Launch Description Titan Robot uses a single bringup launch file that initializes all core components. The bringup launch typically starts: Robot description (URDF) State publisher LIDAR driver ESP32 serial communication node Static and dynamic TFs Launching the Robot Power on the robot and run the bringup launch file: ros2 launch titan_bringup titan_bringup.launch.py Once launched, the following should occur: LIDAR begins publishing scan data ESP32 establishes serial communication Odometry is published to /odom TF tree is available RViz2 opens with a preconfigured view Verifying Bringup After launching, verify that all core components are running correctly. Check Active Topics ros2 topic list You should see topics such as: /scan /odom /cmd_vel /tf /tf_static Verify Odometry ros2 topic echo /odom Values should update when the robot moves Position and orientation should change smoothly Verify LIDAR Data ros2 topic echo /scan Or visualize directly in RViz2. Verify TF Tree ros2 run tf2_tools view_frames Confirm that frames such as odom , base_link , and laser are present. Common Bringup Issues Issue Possible Cause No odometry ESP32 not connected or firmware not running No LIDAR data USB connection or driver issue Robot not moving Motor power or serial issue RViz not showing data Incorrect fixed frame or TF missing Keyboard Teleoperation Keyboard teleoperation is useful for testing hardware, mapping environments, and manual control. Run Keyboard Teleop Open a new terminal and source the workspace: source ~/titan_ws/install/setup.bash Run the teleop node: ros2 run teleop_twist_keyboard teleop_twist_keyboard Teleop Controls i Forward k Stop j Rotate Left l Rotate Right , Backward q/z Increase/Decrease Linear Speed w/x Increase/Decrease Angular Speed Velocity commands are published to the /cmd_vel topic and sent to the ESP32. Safety Notes Always keep the robot in an open area during teleoperation Start with low velocity limits Be ready to stop the robot immediately Do not lift the robot while motors are powered Your robot is now up and running \ud83d\ude80","title":"Robot Bringup"},{"location":"bringup/#robot-bringup","text":"","title":"Robot Bringup \ud83e\udd16"},{"location":"bringup/#overview","text":"This section explains how to build, launch, and verify the complete software stack of Titan Robot . Robot bringup refers to starting all essential components required for the robot to operate correctly, including sensors, motor control, state publishing, and visualization.","title":"Overview"},{"location":"bringup/#prerequisites","text":"Before proceeding, ensure that: Ubuntu 22.04 is installed ROS 2 Humble is installed and sourced The robot hardware is fully assembled Battery is charged and power system is functional ESP32 firmware is already flashed All sensors are properly connected","title":"Prerequisites"},{"location":"bringup/#update-system-packages-and-install-essential-tools","text":"sudo apt update && sudo apt upgrade -y sudo apt install -y curl wget htop net-tools openssh-server rsync tmux python3 python3-pip python3-rosdep gazebo git python3-colcon-common-extensions terminator sudo apt install ros-humble-rviz2 ros-humble-slam-toolbox ros-humble-turtlebot3-gazebo ros-humble-joint-state-publisher-gui ros-humble-gazebo-ros-pkgs","title":"Update system packages and install essential tools"},{"location":"bringup/#one-time-setup","text":"echo 'export TURTLEBOT3_MODEL=burger' >> ~/.bashrc echo 'source /usr/share/gazebo/setup.sh' >> ~/.bashrc echo 'export GAZEBO_MODEL_PATH=~/titan_ws/src/titan_simulation/turtlebot3_gazebo/models:$GAZEBO_MODEL_PATH' >> ~/.bashrc echo 'source ~/titan_ws/install/setup.bash' >> ~/.bashrc","title":"One time setup"},{"location":"bringup/#workspace-setup","text":"Create and initialize a ROS 2 workspace: mkdir -p ~/titan_ws cd ~/titan_ws Clone the Titan Robot repository: git clone https://github.com/MRS111-OS/titan_robot.git mv titan_robot/ src Install dependencies sudo rosdep init rosdep update rosdep install --from-paths src --ignore-src -y Build the workspace: cd ~/titan_ws colcon build --symlink-install --parallel-workers 4 Source the workspace: source install/setup.bash It is recommended to add the source command to your .bashrc for convenience.","title":"Workspace Setup"},{"location":"bringup/#bringup-launch-description","text":"Titan Robot uses a single bringup launch file that initializes all core components. The bringup launch typically starts: Robot description (URDF) State publisher LIDAR driver ESP32 serial communication node Static and dynamic TFs","title":"Bringup Launch Description"},{"location":"bringup/#launching-the-robot","text":"Power on the robot and run the bringup launch file: ros2 launch titan_bringup titan_bringup.launch.py Once launched, the following should occur: LIDAR begins publishing scan data ESP32 establishes serial communication Odometry is published to /odom TF tree is available RViz2 opens with a preconfigured view","title":"Launching the Robot"},{"location":"bringup/#verifying-bringup","text":"After launching, verify that all core components are running correctly.","title":"Verifying Bringup"},{"location":"bringup/#check-active-topics","text":"ros2 topic list You should see topics such as: /scan /odom /cmd_vel /tf /tf_static","title":"Check Active Topics"},{"location":"bringup/#verify-odometry","text":"ros2 topic echo /odom Values should update when the robot moves Position and orientation should change smoothly","title":"Verify Odometry"},{"location":"bringup/#verify-lidar-data","text":"ros2 topic echo /scan Or visualize directly in RViz2.","title":"Verify LIDAR Data"},{"location":"bringup/#verify-tf-tree","text":"ros2 run tf2_tools view_frames Confirm that frames such as odom , base_link , and laser are present.","title":"Verify TF Tree"},{"location":"bringup/#common-bringup-issues","text":"Issue Possible Cause No odometry ESP32 not connected or firmware not running No LIDAR data USB connection or driver issue Robot not moving Motor power or serial issue RViz not showing data Incorrect fixed frame or TF missing","title":"Common Bringup Issues"},{"location":"bringup/#keyboard-teleoperation","text":"Keyboard teleoperation is useful for testing hardware, mapping environments, and manual control.","title":"Keyboard Teleoperation"},{"location":"bringup/#run-keyboard-teleop","text":"Open a new terminal and source the workspace: source ~/titan_ws/install/setup.bash Run the teleop node: ros2 run teleop_twist_keyboard teleop_twist_keyboard","title":"Run Keyboard Teleop"},{"location":"bringup/#teleop-controls","text":"i Forward k Stop j Rotate Left l Rotate Right , Backward q/z Increase/Decrease Linear Speed w/x Increase/Decrease Angular Speed Velocity commands are published to the /cmd_vel topic and sent to the ESP32.","title":"Teleop Controls"},{"location":"bringup/#safety-notes","text":"Always keep the robot in an open area during teleoperation Start with low velocity limits Be ready to stop the robot immediately Do not lift the robot while motors are powered Your robot is now up and running \ud83d\ude80","title":"Safety Notes"},{"location":"docker/","text":"Titan Simulation (Docker) Overview The Titan Simulation Docker provides a ready-to-use environment for running Titan Robot simulation without installing ROS 2, Gazebo, or dependencies on the host system. It is intended for quick demonstrations, behavior validation, and first-time exploration of the Titan navigation stack. Running the Docker Image Run the following commands on the host system: xhost +local:root sudo docker run -it \\ --net=host \\ --device=/dev/dri \\ -e DISPLAY=$DISPLAY \\ -e QT_X11_NO_MITSHM=1 \\ -v /tmp/.X11-unix:/tmp/.X11-unix:rw \\ yashnair02/titan_docker:final What This Command Does Pulls the Docker image automatically if it does not already exist on the system Enables GUI forwarding so Ignition Gazebo and RViz2 can display on the host Uses host networking for seamless ROS 2 and Gazebo communication Provides GPU access for hardware-accelerated rendering Container Startup Behavior When the container starts, a script automatically launches a tmux session with four panes in a single terminal window . The layout is: Top-left pane: Simulation bringup Bottom-left pane: Navigation stack Top-right pane: Idle (user commands) Bottom-right pane: Idle (user commands) Automatic Launch Sequence Inside the container: The Titan simulation bringup is launched automatically Ignition Gazebo starts with the warehouse world The robot is spawned in simulation Nav2 is launched with simulation time enabled At this point, the user only needs to: Open RViz2 Set an initial pose if required Send a 2D Goal Pose The robot will immediately begin autonomous navigation. This setup is intended to quickly demonstrate robot behavior without manual setup. Custom Usage If users want to experiment further, they can: Stop running nodes in any tmux pane Launch SLAM for mapping Load custom maps Tune navigation parameters This Docker environment supports the full simulation workflow described in the Simulation documentation . Navigating tmux Panes To move between tmux panes: Press Ctrl + b, then use arrow keys to switch panes Additional useful commands: Ctrl + b, then d \u2192 Detach from tmux tmux attach \u2192 Reattach to the session Ctrl + d \u2192 Close a pane Further Changes To make further changes in the docker image: 1. Run the image and make changes in the container. 2. List the cuurent containers sudo docker ps Note the container ID Then docker commit container_id username/repository_name:tag_name . This creates a new image called username/repository_name:tag_name If you want to push the image to your personal docker hub, docker push username/repository_name:tag_name Use Case Summary This Docker setup is ideal for: Quick demonstrations Behavior visualization Testing navigation without hardware Sharing a reproducible simulation environment No local ROS 2 or Gazebo installation is required.","title":"Titan Docker Simulation"},{"location":"docker/#titan-simulation-docker","text":"","title":"Titan Simulation (Docker)"},{"location":"docker/#overview","text":"The Titan Simulation Docker provides a ready-to-use environment for running Titan Robot simulation without installing ROS 2, Gazebo, or dependencies on the host system. It is intended for quick demonstrations, behavior validation, and first-time exploration of the Titan navigation stack.","title":"Overview"},{"location":"docker/#running-the-docker-image","text":"Run the following commands on the host system: xhost +local:root sudo docker run -it \\ --net=host \\ --device=/dev/dri \\ -e DISPLAY=$DISPLAY \\ -e QT_X11_NO_MITSHM=1 \\ -v /tmp/.X11-unix:/tmp/.X11-unix:rw \\ yashnair02/titan_docker:final","title":"Running the Docker Image"},{"location":"docker/#what-this-command-does","text":"Pulls the Docker image automatically if it does not already exist on the system Enables GUI forwarding so Ignition Gazebo and RViz2 can display on the host Uses host networking for seamless ROS 2 and Gazebo communication Provides GPU access for hardware-accelerated rendering","title":"What This Command Does"},{"location":"docker/#container-startup-behavior","text":"When the container starts, a script automatically launches a tmux session with four panes in a single terminal window . The layout is: Top-left pane: Simulation bringup Bottom-left pane: Navigation stack Top-right pane: Idle (user commands) Bottom-right pane: Idle (user commands)","title":"Container Startup Behavior"},{"location":"docker/#automatic-launch-sequence","text":"Inside the container: The Titan simulation bringup is launched automatically Ignition Gazebo starts with the warehouse world The robot is spawned in simulation Nav2 is launched with simulation time enabled At this point, the user only needs to: Open RViz2 Set an initial pose if required Send a 2D Goal Pose The robot will immediately begin autonomous navigation. This setup is intended to quickly demonstrate robot behavior without manual setup.","title":"Automatic Launch Sequence"},{"location":"docker/#custom-usage","text":"If users want to experiment further, they can: Stop running nodes in any tmux pane Launch SLAM for mapping Load custom maps Tune navigation parameters This Docker environment supports the full simulation workflow described in the Simulation documentation .","title":"Custom Usage"},{"location":"docker/#navigating-tmux-panes","text":"To move between tmux panes: Press Ctrl + b, then use arrow keys to switch panes Additional useful commands: Ctrl + b, then d \u2192 Detach from tmux tmux attach \u2192 Reattach to the session Ctrl + d \u2192 Close a pane","title":"Navigating tmux Panes"},{"location":"docker/#further-changes","text":"To make further changes in the docker image: 1. Run the image and make changes in the container. 2. List the cuurent containers sudo docker ps Note the container ID Then docker commit container_id username/repository_name:tag_name . This creates a new image called username/repository_name:tag_name If you want to push the image to your personal docker hub, docker push username/repository_name:tag_name","title":"Further Changes"},{"location":"docker/#use-case-summary","text":"This Docker setup is ideal for: Quick demonstrations Behavior visualization Testing navigation without hardware Sharing a reproducible simulation environment No local ROS 2 or Gazebo installation is required.","title":"Use Case Summary"},{"location":"getting-started/","text":"Getting Started Overview This section guides you through the basic requirements and preparation needed to work with Titan Robot . By the end of this section, you will understand the system requirements, robot variants, and how to choose the correct configuration for your use case. System Requirements Titan Robot is designed to run natively on Ubuntu 22.04 LTS with ROS 2 Humble Hawksbill . Supported Operating System Ubuntu 22.04 LTS (64-bit) Required Software ROS 2 Humble Hawksbill (Desktop or Desktop Full) Python 3.10+ It is recommended to install the full ROS 2 desktop version to ensure compatibility with visualization and navigation tools. Supported Robot Variants Titan Robot is available in three hardware variants. Each variant builds on the previous one, adding new sensing and perception capabilities. V1 \u2013 LIDAR-Based Navigation Primary Use Case: Basic autonomous navigation and SLAM Features: - Differential drive mobile base - 2D LIDAR for mapping and obstacle detection - Wheel encoder\u2013based odometry - ROS 2 Navigation Stack support - Keyboard or joystick teleoperation Typical Applications: - Learning ROS 2 navigation - Mapping and localization experiments - Indoor autonomous navigation V2 \u2013 LIDAR + Vision (Pi Camera) Primary Use Case: Autonomous navigation with visual object detection Additional Features over V1: - Raspberry Pi Camera module - Vision-based object detection - Camera integration with ROS 2 image pipeline Typical Applications: - Object detection and tracking - Vision-based robotics projects - AI perception experiments V3 \u2013 LIDAR + Depth Vision (Intel RealSense) Primary Use Case: Advanced perception and 3D-aware navigation Additional Features over V2: - Intel RealSense depth camera - RGB-D perception - Depth-based obstacle understanding - Enhanced perception for autonomy Typical Applications: - 3D perception research - Depth-aware navigation - Advanced autonomous robotics projects Choosing the Right Variant Variant LIDAR Camera Depth Use Case V1 \u2713 \u2717 \u2717 Navigation & SLAM V2 \u2713 Pi Camera \u2717 Navigation + Object Detection V3 \u2713 RealSense \u2713 Advanced Perception & Autonomy Choose the variant that best matches your learning goals or project requirements. You\u2019re now ready to begin working with Titan Robot \ud83d\ude80","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"getting-started/#overview","text":"This section guides you through the basic requirements and preparation needed to work with Titan Robot . By the end of this section, you will understand the system requirements, robot variants, and how to choose the correct configuration for your use case.","title":"Overview"},{"location":"getting-started/#system-requirements","text":"Titan Robot is designed to run natively on Ubuntu 22.04 LTS with ROS 2 Humble Hawksbill .","title":"System Requirements"},{"location":"getting-started/#supported-operating-system","text":"Ubuntu 22.04 LTS (64-bit)","title":"Supported Operating System"},{"location":"getting-started/#required-software","text":"ROS 2 Humble Hawksbill (Desktop or Desktop Full) Python 3.10+ It is recommended to install the full ROS 2 desktop version to ensure compatibility with visualization and navigation tools.","title":"Required Software"},{"location":"getting-started/#supported-robot-variants","text":"Titan Robot is available in three hardware variants. Each variant builds on the previous one, adding new sensing and perception capabilities.","title":"Supported Robot Variants"},{"location":"getting-started/#v1-lidar-based-navigation","text":"Primary Use Case: Basic autonomous navigation and SLAM Features: - Differential drive mobile base - 2D LIDAR for mapping and obstacle detection - Wheel encoder\u2013based odometry - ROS 2 Navigation Stack support - Keyboard or joystick teleoperation Typical Applications: - Learning ROS 2 navigation - Mapping and localization experiments - Indoor autonomous navigation","title":"V1 \u2013 LIDAR-Based Navigation"},{"location":"getting-started/#v2-lidar-vision-pi-camera","text":"Primary Use Case: Autonomous navigation with visual object detection Additional Features over V1: - Raspberry Pi Camera module - Vision-based object detection - Camera integration with ROS 2 image pipeline Typical Applications: - Object detection and tracking - Vision-based robotics projects - AI perception experiments","title":"V2 \u2013 LIDAR + Vision (Pi Camera)"},{"location":"getting-started/#v3-lidar-depth-vision-intel-realsense","text":"Primary Use Case: Advanced perception and 3D-aware navigation Additional Features over V2: - Intel RealSense depth camera - RGB-D perception - Depth-based obstacle understanding - Enhanced perception for autonomy Typical Applications: - 3D perception research - Depth-aware navigation - Advanced autonomous robotics projects","title":"V3 \u2013 LIDAR + Depth Vision (Intel RealSense)"},{"location":"getting-started/#choosing-the-right-variant","text":"Variant LIDAR Camera Depth Use Case V1 \u2713 \u2717 \u2717 Navigation & SLAM V2 \u2713 Pi Camera \u2717 Navigation + Object Detection V3 \u2713 RealSense \u2713 Advanced Perception & Autonomy Choose the variant that best matches your learning goals or project requirements. You\u2019re now ready to begin working with Titan Robot \ud83d\ude80","title":"Choosing the Right Variant"},{"location":"hardware/","text":"Hardware Overview Overview This section describes the complete hardware architecture of Titan Robot , including its mechanical structure, compute units, sensors, electronics, and power system. Hardware Architecture Titan Robot follows a layered hardware design: Compute Layer \u2013 Runs high-level ROS 2 software Control Layer \u2013 Handles real-time motor control and odometry Sensor Layer \u2013 Provides perception and environment awareness Power Layer \u2013 Supplies regulated power to all subsystems Each layer is designed to be independent and modular. Mechanical Design Chassis Differential-drive mobile base Rigid frame with mounting points for: Compute unit Battery Motor controller Sensors Passive caster wheel for balance Compact footprint for indoor navigation Drive System Two DC gear motors Quadrature wheel encoders Rubber wheels for traction Encoder feedback used for odometry and closed-loop control Compute Unit Main Controller Raspberry Pi 4B (8GB RAM) Runs Ubuntu 22.04 LTS Hosts all ROS 2 nodes Handles: SLAM and localization Navigation and planning Perception pipelines User interfaces and visualization Connectivity USB interfaces for sensors and microcontroller WiFi and Ethernet for remote access GPIO headers for expansion Control Electronics Motor Controller (Custom Momentum Robotics Compute Board) Custom Momentum Robotics Compute board based on ESP32 S3 Purpose-built for low-level motor control and real-time operations ESP32 S3 MCU for real-time control Responsibilities: Motor PWM control PID velocity control Encoder reading Odometry computation Communicates with the compute unit via serial interface Publishes odometry and receives velocity commands through ROS 2 Sensor Suite 2D LIDAR 360\u00b0 scanning range Used for: Mapping (SLAM) Localization Obstacle detection Publishes laser scan data to ROS 2 Camera Systems The camera configuration depends on the robot variant: Variant V1 No camera Navigation relies solely on LIDAR and odometry Variant V2 Raspberry Pi Camera Used for: Object detection Vision-based perception Image processing pipelines Variant V3 Intel RealSense depth camera Provides: RGB images Depth data 3D perception capabilities Power System Battery 11.1V 2500mAh NMC (Lithium Nickel Manganese Cobalt Oxide) Rechargeable lithium-based battery Provides sufficient capacity for extended operation Mounted securely within the chassis Power Distribution Main battery output distributed to: Motor driver circuitry Compute unit via voltage regulator Sensors and peripherals Separate regulated rails to ensure stable operation Voltage Regulation Step-down regulators used to supply: 5V for compute unit and logic Motor voltage as required by motors Designed to handle peak current during motor startup Power Management & Safety Power switch for safe startup and shutdown Inline protection (fuse or current limiter) Reverse polarity and overcurrent protection Charging System Dedicated charging port Charging circuit includes: Overvoltage protection Overcurrent protection Thermal safety mechanisms Expansion & Customization Titan Robot is designed for extensibility: USB ports for additional sensors GPIO access for custom electronics Mounting space for: Extra sensors Displays External controllers This makes the platform suitable for both educational use and advanced research. You now have a complete overview of Titan Robot\u2019s hardware platform \ud83d\ude80","title":"Hardware Architecture"},{"location":"hardware/#hardware-overview","text":"","title":"Hardware Overview"},{"location":"hardware/#overview","text":"This section describes the complete hardware architecture of Titan Robot , including its mechanical structure, compute units, sensors, electronics, and power system.","title":"Overview"},{"location":"hardware/#hardware-architecture","text":"Titan Robot follows a layered hardware design: Compute Layer \u2013 Runs high-level ROS 2 software Control Layer \u2013 Handles real-time motor control and odometry Sensor Layer \u2013 Provides perception and environment awareness Power Layer \u2013 Supplies regulated power to all subsystems Each layer is designed to be independent and modular.","title":"Hardware Architecture"},{"location":"hardware/#mechanical-design","text":"","title":"Mechanical Design"},{"location":"hardware/#chassis","text":"Differential-drive mobile base Rigid frame with mounting points for: Compute unit Battery Motor controller Sensors Passive caster wheel for balance Compact footprint for indoor navigation","title":"Chassis"},{"location":"hardware/#drive-system","text":"Two DC gear motors Quadrature wheel encoders Rubber wheels for traction Encoder feedback used for odometry and closed-loop control","title":"Drive System"},{"location":"hardware/#compute-unit","text":"","title":"Compute Unit"},{"location":"hardware/#main-controller","text":"Raspberry Pi 4B (8GB RAM) Runs Ubuntu 22.04 LTS Hosts all ROS 2 nodes Handles: SLAM and localization Navigation and planning Perception pipelines User interfaces and visualization","title":"Main Controller"},{"location":"hardware/#connectivity","text":"USB interfaces for sensors and microcontroller WiFi and Ethernet for remote access GPIO headers for expansion","title":"Connectivity"},{"location":"hardware/#control-electronics","text":"","title":"Control Electronics"},{"location":"hardware/#motor-controller-custom-momentum-robotics-compute-board","text":"Custom Momentum Robotics Compute board based on ESP32 S3 Purpose-built for low-level motor control and real-time operations ESP32 S3 MCU for real-time control Responsibilities: Motor PWM control PID velocity control Encoder reading Odometry computation Communicates with the compute unit via serial interface Publishes odometry and receives velocity commands through ROS 2","title":"Motor Controller (Custom Momentum Robotics Compute Board)"},{"location":"hardware/#sensor-suite","text":"","title":"Sensor Suite"},{"location":"hardware/#2d-lidar","text":"360\u00b0 scanning range Used for: Mapping (SLAM) Localization Obstacle detection Publishes laser scan data to ROS 2","title":"2D LIDAR"},{"location":"hardware/#camera-systems","text":"The camera configuration depends on the robot variant:","title":"Camera Systems"},{"location":"hardware/#variant-v1","text":"No camera Navigation relies solely on LIDAR and odometry","title":"Variant V1"},{"location":"hardware/#variant-v2","text":"Raspberry Pi Camera Used for: Object detection Vision-based perception Image processing pipelines","title":"Variant V2"},{"location":"hardware/#variant-v3","text":"Intel RealSense depth camera Provides: RGB images Depth data 3D perception capabilities","title":"Variant V3"},{"location":"hardware/#power-system","text":"","title":"Power System"},{"location":"hardware/#battery","text":"11.1V 2500mAh NMC (Lithium Nickel Manganese Cobalt Oxide) Rechargeable lithium-based battery Provides sufficient capacity for extended operation Mounted securely within the chassis","title":"Battery"},{"location":"hardware/#power-distribution","text":"Main battery output distributed to: Motor driver circuitry Compute unit via voltage regulator Sensors and peripherals Separate regulated rails to ensure stable operation","title":"Power Distribution"},{"location":"hardware/#voltage-regulation","text":"Step-down regulators used to supply: 5V for compute unit and logic Motor voltage as required by motors Designed to handle peak current during motor startup","title":"Voltage Regulation"},{"location":"hardware/#power-management-safety","text":"Power switch for safe startup and shutdown Inline protection (fuse or current limiter) Reverse polarity and overcurrent protection","title":"Power Management &amp; Safety"},{"location":"hardware/#charging-system","text":"Dedicated charging port Charging circuit includes: Overvoltage protection Overcurrent protection Thermal safety mechanisms","title":"Charging System"},{"location":"hardware/#expansion-customization","text":"Titan Robot is designed for extensibility: USB ports for additional sensors GPIO access for custom electronics Mounting space for: Extra sensors Displays External controllers This makes the platform suitable for both educational use and advanced research. You now have a complete overview of Titan Robot\u2019s hardware platform \ud83d\ude80","title":"Expansion &amp; Customization"},{"location":"mapping/","text":"Mapping & SLAM Overview Mapping and SLAM (Simultaneous Localization and Mapping) enable Titan Robot to understand and navigate unknown environments. This section explains how to create maps using SLAM Toolbox , how mapping works internally, and best practices for generating high-quality maps. Titan Robot performs 2D LIDAR-based SLAM using wheel odometry and laser scan data. SLAM Concepts What is SLAM? SLAM allows a robot to: - Build a map of an unknown environment - Estimate its own position within that map at the same time Titan Robot uses: - Laser scans from the 2D LIDAR - Odometry from wheel encoders These inputs are fused by SLAM Toolbox to continuously update the map. Inputs Used for SLAM Input Source Laser scans 2D LIDAR ( /scan ) Odometry ESP32 encoder-based odometry ( /odom ) TF Robot frame transforms SLAM Toolbox Overview SLAM Toolbox is the primary SLAM solution used on Titan Robot. It supports: - Online asynchronous mapping - Map serialization - Localization mode using saved maps SLAM Toolbox is preferred due to: - Robust loop closure - ROS 2 native support - Efficient performance on embedded computers Starting Mapping Mode Launch Required Components Ensure the robot is powered on and bringup is running: ros2 launch titan_bringup titan_bringup.launch.py Launch SLAM Toolbox (Mapping Mode) In a new terminal: ros2 launch slam_toolbox online_async_launch.py This starts SLAM Toolbox in online mapping mode . Visualizing the Map in RViz2 In RViz2: Set Fixed Frame to map Add the following displays: Map LaserScan TF RobotModel You should see the map being generated in real time as the robot moves. Exploring the Environment Use keyboard teleoperation to explore: ros2 run teleop_twist_keyboard teleop_twist_keyboard Mapping Tips Move slowly and smoothly Avoid rapid rotations Ensure overlap between scans Cover all reachable areas Avoid bumping into obstacles Good exploration significantly improves map quality. Saving the Map Once mapping is complete, save the map using RViz2: In RViz2, click Panels \u2192 Add New Panel Select SlamToolboxPlugin Click Save Map Choose a directory and map name This generates: - <map_name>.yaml - <map_name>.pgm Map Files Explained PGM file \u2013 Occupancy grid image YAML file \u2013 Metadata describing: Resolution Origin Map image path Both files are required for localization and navigation. Common Mapping Issues Issue Possible Cause Recommended Solution Map drift over time Inaccurate wheel odometry Calibrate wheel encoders and odometry parameters Wheel slip on smooth surfaces Reduce speed and improve wheel traction Fast or abrupt robot motion Move slowly and avoid sudden turns Poor map quality Incomplete environment coverage Ensure full exploration of the area Rapid rotations Rotate slowly to maintain scan alignment Sensor noise or vibration Secure LIDAR mounting and reduce vibration No map visible in RViz Incorrect fixed frame Set RViz fixed frame to map SLAM Toolbox not running Verify SLAM node is active Broken or distorted walls Odometry error accumulation Improve encoder resolution and tuning Inconsistent scan overlap Maintain steady movement with good scan overlap Loop closure failure Insufficient environmental features Map areas with distinctive geometry Poor revisit alignment Revisit locations slowly and from similar angles Performance Considerations Mapping performance depends on: LIDAR scan rate Odometry accuracy CPU load Close unnecessary applications during mapping Avoid running heavy perception pipelines simultaneously When to Remap You should create a new map if: - Environment layout changes significantly - LIDAR mounting position changes - Wheel parameters are updated - Odometry calibration is modified You have now successfully created a map using Titan Robot \ud83d\ude80","title":"Mapping"},{"location":"mapping/#mapping-slam","text":"","title":"Mapping &amp; SLAM"},{"location":"mapping/#overview","text":"Mapping and SLAM (Simultaneous Localization and Mapping) enable Titan Robot to understand and navigate unknown environments. This section explains how to create maps using SLAM Toolbox , how mapping works internally, and best practices for generating high-quality maps. Titan Robot performs 2D LIDAR-based SLAM using wheel odometry and laser scan data.","title":"Overview"},{"location":"mapping/#slam-concepts","text":"","title":"SLAM Concepts"},{"location":"mapping/#what-is-slam","text":"SLAM allows a robot to: - Build a map of an unknown environment - Estimate its own position within that map at the same time Titan Robot uses: - Laser scans from the 2D LIDAR - Odometry from wheel encoders These inputs are fused by SLAM Toolbox to continuously update the map.","title":"What is SLAM?"},{"location":"mapping/#inputs-used-for-slam","text":"Input Source Laser scans 2D LIDAR ( /scan ) Odometry ESP32 encoder-based odometry ( /odom ) TF Robot frame transforms","title":"Inputs Used for SLAM"},{"location":"mapping/#slam-toolbox-overview","text":"SLAM Toolbox is the primary SLAM solution used on Titan Robot. It supports: - Online asynchronous mapping - Map serialization - Localization mode using saved maps SLAM Toolbox is preferred due to: - Robust loop closure - ROS 2 native support - Efficient performance on embedded computers","title":"SLAM Toolbox Overview"},{"location":"mapping/#starting-mapping-mode","text":"","title":"Starting Mapping Mode"},{"location":"mapping/#launch-required-components","text":"Ensure the robot is powered on and bringup is running: ros2 launch titan_bringup titan_bringup.launch.py","title":"Launch Required Components"},{"location":"mapping/#launch-slam-toolbox-mapping-mode","text":"In a new terminal: ros2 launch slam_toolbox online_async_launch.py This starts SLAM Toolbox in online mapping mode .","title":"Launch SLAM Toolbox (Mapping Mode)"},{"location":"mapping/#visualizing-the-map-in-rviz2","text":"In RViz2: Set Fixed Frame to map Add the following displays: Map LaserScan TF RobotModel You should see the map being generated in real time as the robot moves.","title":"Visualizing the Map in RViz2"},{"location":"mapping/#exploring-the-environment","text":"Use keyboard teleoperation to explore: ros2 run teleop_twist_keyboard teleop_twist_keyboard","title":"Exploring the Environment"},{"location":"mapping/#mapping-tips","text":"Move slowly and smoothly Avoid rapid rotations Ensure overlap between scans Cover all reachable areas Avoid bumping into obstacles Good exploration significantly improves map quality.","title":"Mapping Tips"},{"location":"mapping/#saving-the-map","text":"Once mapping is complete, save the map using RViz2: In RViz2, click Panels \u2192 Add New Panel Select SlamToolboxPlugin Click Save Map Choose a directory and map name This generates: - <map_name>.yaml - <map_name>.pgm","title":"Saving the Map"},{"location":"mapping/#map-files-explained","text":"PGM file \u2013 Occupancy grid image YAML file \u2013 Metadata describing: Resolution Origin Map image path Both files are required for localization and navigation.","title":"Map Files Explained"},{"location":"mapping/#common-mapping-issues","text":"Issue Possible Cause Recommended Solution Map drift over time Inaccurate wheel odometry Calibrate wheel encoders and odometry parameters Wheel slip on smooth surfaces Reduce speed and improve wheel traction Fast or abrupt robot motion Move slowly and avoid sudden turns Poor map quality Incomplete environment coverage Ensure full exploration of the area Rapid rotations Rotate slowly to maintain scan alignment Sensor noise or vibration Secure LIDAR mounting and reduce vibration No map visible in RViz Incorrect fixed frame Set RViz fixed frame to map SLAM Toolbox not running Verify SLAM node is active Broken or distorted walls Odometry error accumulation Improve encoder resolution and tuning Inconsistent scan overlap Maintain steady movement with good scan overlap Loop closure failure Insufficient environmental features Map areas with distinctive geometry Poor revisit alignment Revisit locations slowly and from similar angles","title":"Common Mapping Issues"},{"location":"mapping/#performance-considerations","text":"Mapping performance depends on: LIDAR scan rate Odometry accuracy CPU load Close unnecessary applications during mapping Avoid running heavy perception pipelines simultaneously","title":"Performance Considerations"},{"location":"mapping/#when-to-remap","text":"You should create a new map if: - Environment layout changes significantly - LIDAR mounting position changes - Wheel parameters are updated - Odometry calibration is modified You have now successfully created a map using Titan Robot \ud83d\ude80","title":"When to Remap"},{"location":"navigation/","text":"Localization & Navigation Overview Localization and navigation allow Titan Robot to estimate its position within a known map and autonomously move to target locations. This section explains how localization and navigation are performed using ROS 2 Navigation (Nav2) . Titan Robot relies entirely on Nav2 for: - Localization using a pre-built map - Global and local path planning - Obstacle avoidance - Velocity command generation Localization Concepts What is Localization? Localization is the process of estimating the robot\u2019s pose (x, y, \u03b8) within a known map. Once a map is provided, the robot continuously estimates its position using sensor data and odometry. Nav2 uses a probabilistic localization approach (AMCL) to localize the robot on the map. Inputs Used for Localization Input Source Map Pre-built occupancy grid Laser scans 2D LIDAR ( /scan ) Odometry Encoder-based odometry ( /odom ) TF Coordinate frame transforms Prerequisites Before starting localization and navigation, ensure that: A valid map file (.yaml and .pgm) is available Robot bringup is running LIDAR is publishing scan data Odometry is being published correctly Launching Localization and Navigation Localization and navigation are launched together using Nav2 bringup. Run the following command: ros2 launch nav2_bringup bringup.launch.py map:=/absolute/path/to/map.yaml This launch file starts: - Map server - Localization (AMCL) - Global planner - Local controller - Costmaps - Behavior tree navigator Initial Pose Estimation After Nav2 starts, the robot must be given an initial pose. In RViz2: Set Fixed Frame to map Select the 2D Pose Estimate tool Click on the robot\u2019s approximate position on the map Set the robot\u2019s orientation The localization system will converge to the correct pose within a few seconds. Verifying Localization Localization is working correctly if: The robot model aligns with the map Laser scans match map walls Small movements update the pose smoothly The pose does not jump unexpectedly Navigation Overview Navigation allows the robot to autonomously move to a target pose while avoiding obstacles. Nav2 handles: - Global path planning - Local obstacle avoidance - Velocity command generation Velocity commands are published to the /cmd_vel topic and sent to the ESP32. Planners Global Planner Evaluation (Nav2) This document presents a structured evaluation of different global planners available in ROS 2 Nav2. The planners are compared using quantitative path metrics and qualitative visual inspection to understand their behavior during navigation. Global Planner Overview A global planner is responsible for generating a collision-free path from the robot\u2019s current pose to a goal pose using the global costmap. The output of the planner is a discrete sequence of waypoints that the local controller later tracks. The quality of this path directly affects navigation efficiency, smoothness, and controller performance. Planners Evaluated NavFn NavFn is a classical grid-based planner that computes paths using Dijkstra\u2019s algorithm over the global costmap. It guarantees a cost-optimal solution with respect to the costmap but restricts motion to grid-aligned directions. Due to this grid constraint, paths often contain sharp turns and unnecessary zig-zag behavior. Smac Planner 2D Smac Planner 2D is a search-based A* planner that uses cost heuristics and motion primitives. It expands the state space beyond simple grid neighbors, allowing smoother transitions and more kinematically feasible paths. The planner balances optimality with execution feasibility, making it well suited for differential-drive robots. Theta* Theta is an extension of the A algorithm that enables any-angle planning by allowing parent reassignment through line-of-sight checks. Instead of being constrained to grid directions, the planner directly connects nodes when a clear path exists. This typically results in shorter and smoother paths with fewer heading changes. Planner Evaluation Metrics Path Length Path length represents the total geometric distance of the planned path from the robot\u2019s start position to the goal. It is computed as the sum of Euclidean distances between consecutive waypoints along the global plan. This metric provides a direct indication of how efficiently the planner connects the start and goal positions. Shorter paths generally imply reduced travel distance and potentially lower execution time, assuming the controller can track the path effectively. Path Smoothness Path smoothness captures how much the direction of motion changes along the planned path. It is calculated by summing the absolute angular differences between consecutive path segments, expressed in radians. A smoother path exhibits gradual heading changes, which reduces steering effort and makes the path easier for local controllers to follow. Higher smoothness values indicate frequent or sharp directional changes, often arising from grid constraints or obstacle avoidance behavior. Replan ID The replan ID is an index assigned to each new global path generated during navigation. As the robot moves and the planner replans, this index increments sequentially. Tracking metrics against the replan ID allows observation of how path quality evolves as the robot approaches the goal and the remaining path shortens. Experimental Results Path Length Comparison The path length comparison shows a consistent reduction in path length as the robot progresses toward the goal. SmacPlanner 2D generally produces the shortest paths due to its any-angle planning capability, allowing direct connections when line-of-sight exists. Smac Planner 2D follows closely, while NavFn typically generates slightly longer paths because of its grid-aligned expansion. Path Smoothness Comparison The smoothness comparison reveals that Smac Planner 2D produces the straightest and most consistent paths overall. NavFn follows with moderate smoothness, while Theta* exhibits higher variation due to occasional aggressive shortcuts and line-of-sight connections that introduce sharper angular transitions. This behavior becomes more visible during replanning phases as the robot nears the goal. Summary Theta* produces the shortest paths but shows higher variability in smoothness. Smac Planner 2D generates the smoothest and most stable paths across replans. NavFn provides predictable behavior but remains constrained by grid-based motion. Controllers Local Controller Evaluation (Nav2) This document presents a structured evaluation of different local controllers available in ROS 2 Nav2. The controllers are compared using time-based and motion-based metrics to understand how they execute a given global path and drive the robot toward the goal. Local Controller Overview A local controller is responsible for converting a global path into real-time velocity commands ( cmd_vel ) that move the robot safely and efficiently. While the global planner decides where the robot should go, the controller decides how the robot should move at each instant. Controller behavior directly affects motion smoothness, responsiveness, stability, and time to reach the goal. Controllers Evaluated Regulated Pure Pursuit (RPP) Regulated Pure Pursuit is a geometric path-tracking controller based on the pure pursuit algorithm . It selects a lookahead point on the global path and computes curvature commands that steer the robot toward that point. Regulation terms are added to limit velocity near obstacles, sharp turns, or goal proximity. This results in smooth, predictable motion with minimal oscillations, making RPP well suited for structured environments and clean global paths. Dynamic Window Approach (DWB) DWB is a trajectory-sampling controller that evaluates multiple velocity commands in the robot\u2019s dynamic window. Each candidate trajectory is forward-simulated and scored using a set of critics such as path alignment, goal distance, obstacle cost, and rotation penalties. The command with the lowest cost is selected at each control cycle. While highly flexible and reactive, DWB can exhibit oscillatory behavior if not carefully tuned, especially in open or cluttered spaces. Controller Evaluation Metrics Time to Goal Time to goal represents the total duration taken by the robot to reach the navigation goal after a navigation request is issued. It is measured from the moment the controller begins execution until the goal is reported as reached. This metric reflects overall controller efficiency, including responsiveness, stability, and interaction with the planner. Instantaneous Speed Instantaneous speed captures the robot\u2019s linear velocity at each control timestep. It is obtained directly from velocity commands ( cmd_vel ) published by the controller, allowing fine-grained observation of motion behavior. Tracking instantaneous speed over time reveals how aggressively or conservatively a controller drives the robot and how it responds to path curvature and goal proximity. Acceleration Acceleration is derived from the change in instantaneous speed over time. It provides insight into motion smoothness and dynamic stability. High acceleration spikes indicate abrupt speed changes, which may lead to wheel slip, poor tracking, or uncomfortable motion in real robots. Experimental Results Time to Goal Comparison The time-to-goal results show that Regulated Pure Pursuit consistently reaches the goal faster in this setup due to its direct path-following behavior and minimal hesitation. DWB, while robust, tends to take longer due to frequent re-evaluation of trajectories and conservative velocity choices near obstacles. Speed Profile Comparison The speed profile comparison highlights a clear behavioral difference between the controllers. RPP maintains a relatively smooth and stable velocity profile, gradually slowing near turns and the goal. DWB shows noticeable fluctuations in speed caused by trajectory switching and critic competition. Summary Regulated Pure Pursuit provides smooth, stable motion with predictable speed behavior and faster goal completion. Dynamic Window Approach offers strong obstacle awareness and flexibility but introduces speed oscillations and longer execution times. For structured environments with reliable global paths, RPP delivers superior execution quality, while DWB remains valuable in highly dynamic or cluttered scenarios. Sending Navigation Goals To send a navigation goal: Open RViz2 Set Fixed Frame to map Select 2D Goal Pose Click on the desired target location Set the target orientation Nav2 will compute a path and drive the robot to the goal. Costmaps Global Costmap Based on the static map Used for long-range path planning Represents static obstacles Local Costmap Built from real-time sensor data Used for obstacle avoidance Updates continuously during navigation Navigation Behavior During navigation, the robot: Plans a path to the goal Continuously updates the path Avoids static and dynamic obstacles Stops when the goal is reached Issues Common Localization Issues Issue Possible Cause Solution Robot pose jumps Incorrect initial pose Reset pose using RViz Slow convergence Odometry drift Improve encoder calibration Laser mismatch Incorrect TF frames Verify LIDAR transforms Common Navigation Issues Issue Possible Cause Solution Robot does not move No velocity commands Check Nav2 nodes and /cmd_vel Oscillations Aggressive controller tuning Tune controller parameters Collisions Incorrect costmap setup Adjust footprint and inflation No path generated Localization failure Verify map and pose accuracy Safety Guidelines Test navigation in open environments Limit maximum speed during initial testing Always monitor the robot during autonomous runs Keep an emergency stop accessible Performance Tuning Tips Ensure odometry is accurate before tuning Nav2 Verify TF tree correctness Reduce costmap resolution if CPU usage is high Avoid running heavy perception nodes simultaneously Your robot is now fully capable of localization and autonomous navigation using Nav2 \ud83d\ude80","title":"Localization and Navigation"},{"location":"navigation/#localization-navigation","text":"","title":"Localization &amp; Navigation"},{"location":"navigation/#overview","text":"Localization and navigation allow Titan Robot to estimate its position within a known map and autonomously move to target locations. This section explains how localization and navigation are performed using ROS 2 Navigation (Nav2) . Titan Robot relies entirely on Nav2 for: - Localization using a pre-built map - Global and local path planning - Obstacle avoidance - Velocity command generation","title":"Overview"},{"location":"navigation/#localization-concepts","text":"","title":"Localization Concepts"},{"location":"navigation/#what-is-localization","text":"Localization is the process of estimating the robot\u2019s pose (x, y, \u03b8) within a known map. Once a map is provided, the robot continuously estimates its position using sensor data and odometry. Nav2 uses a probabilistic localization approach (AMCL) to localize the robot on the map.","title":"What is Localization?"},{"location":"navigation/#inputs-used-for-localization","text":"Input Source Map Pre-built occupancy grid Laser scans 2D LIDAR ( /scan ) Odometry Encoder-based odometry ( /odom ) TF Coordinate frame transforms","title":"Inputs Used for Localization"},{"location":"navigation/#prerequisites","text":"Before starting localization and navigation, ensure that: A valid map file (.yaml and .pgm) is available Robot bringup is running LIDAR is publishing scan data Odometry is being published correctly","title":"Prerequisites"},{"location":"navigation/#launching-localization-and-navigation","text":"Localization and navigation are launched together using Nav2 bringup. Run the following command: ros2 launch nav2_bringup bringup.launch.py map:=/absolute/path/to/map.yaml This launch file starts: - Map server - Localization (AMCL) - Global planner - Local controller - Costmaps - Behavior tree navigator","title":"Launching Localization and Navigation"},{"location":"navigation/#initial-pose-estimation","text":"After Nav2 starts, the robot must be given an initial pose. In RViz2: Set Fixed Frame to map Select the 2D Pose Estimate tool Click on the robot\u2019s approximate position on the map Set the robot\u2019s orientation The localization system will converge to the correct pose within a few seconds.","title":"Initial Pose Estimation"},{"location":"navigation/#verifying-localization","text":"Localization is working correctly if: The robot model aligns with the map Laser scans match map walls Small movements update the pose smoothly The pose does not jump unexpectedly","title":"Verifying Localization"},{"location":"navigation/#navigation-overview","text":"Navigation allows the robot to autonomously move to a target pose while avoiding obstacles. Nav2 handles: - Global path planning - Local obstacle avoidance - Velocity command generation Velocity commands are published to the /cmd_vel topic and sent to the ESP32.","title":"Navigation Overview"},{"location":"navigation/#planners","text":"","title":"Planners"},{"location":"navigation/#global-planner-evaluation-nav2","text":"This document presents a structured evaluation of different global planners available in ROS 2 Nav2. The planners are compared using quantitative path metrics and qualitative visual inspection to understand their behavior during navigation.","title":"Global Planner Evaluation (Nav2)"},{"location":"navigation/#global-planner-overview","text":"A global planner is responsible for generating a collision-free path from the robot\u2019s current pose to a goal pose using the global costmap. The output of the planner is a discrete sequence of waypoints that the local controller later tracks. The quality of this path directly affects navigation efficiency, smoothness, and controller performance.","title":"Global Planner Overview"},{"location":"navigation/#planners-evaluated","text":"","title":"Planners Evaluated"},{"location":"navigation/#navfn","text":"NavFn is a classical grid-based planner that computes paths using Dijkstra\u2019s algorithm over the global costmap. It guarantees a cost-optimal solution with respect to the costmap but restricts motion to grid-aligned directions. Due to this grid constraint, paths often contain sharp turns and unnecessary zig-zag behavior.","title":"NavFn"},{"location":"navigation/#smac-planner-2d","text":"Smac Planner 2D is a search-based A* planner that uses cost heuristics and motion primitives. It expands the state space beyond simple grid neighbors, allowing smoother transitions and more kinematically feasible paths. The planner balances optimality with execution feasibility, making it well suited for differential-drive robots.","title":"Smac Planner 2D"},{"location":"navigation/#theta","text":"Theta is an extension of the A algorithm that enables any-angle planning by allowing parent reassignment through line-of-sight checks. Instead of being constrained to grid directions, the planner directly connects nodes when a clear path exists. This typically results in shorter and smoother paths with fewer heading changes.","title":"Theta*"},{"location":"navigation/#planner-evaluation-metrics","text":"","title":"Planner Evaluation Metrics"},{"location":"navigation/#path-length","text":"Path length represents the total geometric distance of the planned path from the robot\u2019s start position to the goal. It is computed as the sum of Euclidean distances between consecutive waypoints along the global plan. This metric provides a direct indication of how efficiently the planner connects the start and goal positions. Shorter paths generally imply reduced travel distance and potentially lower execution time, assuming the controller can track the path effectively.","title":"Path Length"},{"location":"navigation/#path-smoothness","text":"Path smoothness captures how much the direction of motion changes along the planned path. It is calculated by summing the absolute angular differences between consecutive path segments, expressed in radians. A smoother path exhibits gradual heading changes, which reduces steering effort and makes the path easier for local controllers to follow. Higher smoothness values indicate frequent or sharp directional changes, often arising from grid constraints or obstacle avoidance behavior.","title":"Path Smoothness"},{"location":"navigation/#replan-id","text":"The replan ID is an index assigned to each new global path generated during navigation. As the robot moves and the planner replans, this index increments sequentially. Tracking metrics against the replan ID allows observation of how path quality evolves as the robot approaches the goal and the remaining path shortens.","title":"Replan ID"},{"location":"navigation/#experimental-results","text":"","title":"Experimental Results"},{"location":"navigation/#path-length-comparison","text":"The path length comparison shows a consistent reduction in path length as the robot progresses toward the goal. SmacPlanner 2D generally produces the shortest paths due to its any-angle planning capability, allowing direct connections when line-of-sight exists. Smac Planner 2D follows closely, while NavFn typically generates slightly longer paths because of its grid-aligned expansion.","title":"Path Length Comparison"},{"location":"navigation/#path-smoothness-comparison","text":"The smoothness comparison reveals that Smac Planner 2D produces the straightest and most consistent paths overall. NavFn follows with moderate smoothness, while Theta* exhibits higher variation due to occasional aggressive shortcuts and line-of-sight connections that introduce sharper angular transitions. This behavior becomes more visible during replanning phases as the robot nears the goal.","title":"Path Smoothness Comparison"},{"location":"navigation/#summary","text":"Theta* produces the shortest paths but shows higher variability in smoothness. Smac Planner 2D generates the smoothest and most stable paths across replans. NavFn provides predictable behavior but remains constrained by grid-based motion.","title":"Summary"},{"location":"navigation/#controllers","text":"","title":"Controllers"},{"location":"navigation/#local-controller-evaluation-nav2","text":"This document presents a structured evaluation of different local controllers available in ROS 2 Nav2. The controllers are compared using time-based and motion-based metrics to understand how they execute a given global path and drive the robot toward the goal.","title":"Local Controller Evaluation (Nav2)"},{"location":"navigation/#local-controller-overview","text":"A local controller is responsible for converting a global path into real-time velocity commands ( cmd_vel ) that move the robot safely and efficiently. While the global planner decides where the robot should go, the controller decides how the robot should move at each instant. Controller behavior directly affects motion smoothness, responsiveness, stability, and time to reach the goal.","title":"Local Controller Overview"},{"location":"navigation/#controllers-evaluated","text":"","title":"Controllers Evaluated"},{"location":"navigation/#regulated-pure-pursuit-rpp","text":"Regulated Pure Pursuit is a geometric path-tracking controller based on the pure pursuit algorithm . It selects a lookahead point on the global path and computes curvature commands that steer the robot toward that point. Regulation terms are added to limit velocity near obstacles, sharp turns, or goal proximity. This results in smooth, predictable motion with minimal oscillations, making RPP well suited for structured environments and clean global paths.","title":"Regulated Pure Pursuit (RPP)"},{"location":"navigation/#dynamic-window-approach-dwb","text":"DWB is a trajectory-sampling controller that evaluates multiple velocity commands in the robot\u2019s dynamic window. Each candidate trajectory is forward-simulated and scored using a set of critics such as path alignment, goal distance, obstacle cost, and rotation penalties. The command with the lowest cost is selected at each control cycle. While highly flexible and reactive, DWB can exhibit oscillatory behavior if not carefully tuned, especially in open or cluttered spaces.","title":"Dynamic Window Approach (DWB)"},{"location":"navigation/#controller-evaluation-metrics","text":"","title":"Controller Evaluation Metrics"},{"location":"navigation/#time-to-goal","text":"Time to goal represents the total duration taken by the robot to reach the navigation goal after a navigation request is issued. It is measured from the moment the controller begins execution until the goal is reported as reached. This metric reflects overall controller efficiency, including responsiveness, stability, and interaction with the planner.","title":"Time to Goal"},{"location":"navigation/#instantaneous-speed","text":"Instantaneous speed captures the robot\u2019s linear velocity at each control timestep. It is obtained directly from velocity commands ( cmd_vel ) published by the controller, allowing fine-grained observation of motion behavior. Tracking instantaneous speed over time reveals how aggressively or conservatively a controller drives the robot and how it responds to path curvature and goal proximity.","title":"Instantaneous Speed"},{"location":"navigation/#acceleration","text":"Acceleration is derived from the change in instantaneous speed over time. It provides insight into motion smoothness and dynamic stability. High acceleration spikes indicate abrupt speed changes, which may lead to wheel slip, poor tracking, or uncomfortable motion in real robots.","title":"Acceleration"},{"location":"navigation/#experimental-results_1","text":"","title":"Experimental Results"},{"location":"navigation/#time-to-goal-comparison","text":"The time-to-goal results show that Regulated Pure Pursuit consistently reaches the goal faster in this setup due to its direct path-following behavior and minimal hesitation. DWB, while robust, tends to take longer due to frequent re-evaluation of trajectories and conservative velocity choices near obstacles.","title":"Time to Goal Comparison"},{"location":"navigation/#speed-profile-comparison","text":"The speed profile comparison highlights a clear behavioral difference between the controllers. RPP maintains a relatively smooth and stable velocity profile, gradually slowing near turns and the goal. DWB shows noticeable fluctuations in speed caused by trajectory switching and critic competition.","title":"Speed Profile Comparison"},{"location":"navigation/#summary_1","text":"Regulated Pure Pursuit provides smooth, stable motion with predictable speed behavior and faster goal completion. Dynamic Window Approach offers strong obstacle awareness and flexibility but introduces speed oscillations and longer execution times. For structured environments with reliable global paths, RPP delivers superior execution quality, while DWB remains valuable in highly dynamic or cluttered scenarios.","title":"Summary"},{"location":"navigation/#sending-navigation-goals","text":"To send a navigation goal: Open RViz2 Set Fixed Frame to map Select 2D Goal Pose Click on the desired target location Set the target orientation Nav2 will compute a path and drive the robot to the goal.","title":"Sending Navigation Goals"},{"location":"navigation/#costmaps","text":"","title":"Costmaps"},{"location":"navigation/#global-costmap","text":"Based on the static map Used for long-range path planning Represents static obstacles","title":"Global Costmap"},{"location":"navigation/#local-costmap","text":"Built from real-time sensor data Used for obstacle avoidance Updates continuously during navigation","title":"Local Costmap"},{"location":"navigation/#navigation-behavior","text":"During navigation, the robot: Plans a path to the goal Continuously updates the path Avoids static and dynamic obstacles Stops when the goal is reached","title":"Navigation Behavior"},{"location":"navigation/#issues","text":"","title":"Issues"},{"location":"navigation/#common-localization-issues","text":"Issue Possible Cause Solution Robot pose jumps Incorrect initial pose Reset pose using RViz Slow convergence Odometry drift Improve encoder calibration Laser mismatch Incorrect TF frames Verify LIDAR transforms","title":"Common Localization Issues"},{"location":"navigation/#common-navigation-issues","text":"Issue Possible Cause Solution Robot does not move No velocity commands Check Nav2 nodes and /cmd_vel Oscillations Aggressive controller tuning Tune controller parameters Collisions Incorrect costmap setup Adjust footprint and inflation No path generated Localization failure Verify map and pose accuracy","title":"Common Navigation Issues"},{"location":"navigation/#safety-guidelines","text":"Test navigation in open environments Limit maximum speed during initial testing Always monitor the robot during autonomous runs Keep an emergency stop accessible","title":"Safety Guidelines"},{"location":"navigation/#performance-tuning-tips","text":"Ensure odometry is accurate before tuning Nav2 Verify TF tree correctness Reduce costmap resolution if CPU usage is high Avoid running heavy perception nodes simultaneously Your robot is now fully capable of localization and autonomous navigation using Nav2 \ud83d\ude80","title":"Performance Tuning Tips"},{"location":"networking/","text":"Networking & Remote Access Overview Networking and remote access allow you to control, monitor, and debug Titan Robot without directly connecting a keyboard and monitor. This section explains how to connect the robot to a network, access it remotely, and configure ROS 2 communication across machines. Titan Robot supports: - WiFi and Ethernet networking - SSH-based remote access - Remote RViz visualization - Multi-machine ROS 2 setups Network Requirements Robot and control PC must be on the same network Stable WiFi connection recommended for visualization and debugging Connecting the Robot to WiFi Using Network Manager (Recommended) List available WiFi networks: nmcli device wifi list Connect to a WiFi network: nmcli device wifi connect \"YOUR_SSID\" password \"YOUR_PASSWORD\" Verify connection: ip addr Look for a valid IP address assigned to the wireless interface. Finding the Robot\u2019s IP Address On the robot, run: hostname -I This will display the robot\u2019s IP address. Finding the Robot from Another Device If you do not know the IP address, scan the network: sudo apt install nmap sudo nmap -sn 192.168.1.0/24 Look for the device corresponding to the robot. SSH Remote Access SSH allows you to access the robot\u2019s terminal remotely. Enable SSH (if not already enabled) sudo systemctl enable ssh sudo systemctl start ssh Connecting via SSH From your laptop or desktop: ssh <username>@<robot_ip_address> Example: ssh titan@192.168.1.42 Once connected, you can run ROS 2 commands as if you were physically on the robot. Sourcing ROS 2 After logging in via SSH, ensure ROS 2 is sourced: source /opt/ros/humble/setup.bash source ~/titan_ws/install/setup.bash Remote RViz2 Usage RViz2 can be run either: - On the robot - On a remote PC connected over the network Running RViz2 remotely is recommended to reduce CPU load on the robot. Running RViz2 on a Remote PC Ensure: - ROS 2 Humble is installed on the remote PC - The same workspace (or compatible message definitions) is available Set the same ROS 2 domain ID on both machines. ROS 2 Networking Configuration ROS 2 uses DDS for communication and works automatically when machines are on the same network. ROS_DOMAIN_ID Set the same domain ID on all machines: export ROS_DOMAIN_ID=0 Add this to .bashrc on both robot and remote PC to persist. Verifying ROS 2 Communication On the remote PC: ros2 node list ros2 topic list If nodes and topics from the robot appear, communication is working correctly. Multi-Machine ROS 2 Setup Common setup: - Robot runs sensors, control, and navigation - Remote PC runs RViz2 and debugging tools Ensure: - Same ROS_DOMAIN_ID - Same ROS 2 distribution - Firewall does not block DDS traffic Firewall Considerations If communication issues occur, check firewall settings: sudo ufw status If enabled, allow traffic or temporarily disable for testing: sudo ufw disable Ethernet Connection (Optional) For development and testing: - Connect robot directly to a PC using Ethernet - Assign IP addresses automatically or manually - Provides lower latency and higher reliability than WiFi Security Best Practices Change default passwords Disable unused services Use SSH keys instead of passwords Avoid running robot on public networks Common Networking Issues Issue Possible Cause Solution Cannot SSH Wrong IP or SSH disabled Verify IP and SSH service ROS topics not visible Different ROS_DOMAIN_ID Set same domain ID High latency Weak WiFi signal Move closer or use Ethernet RViz lag Running on robot Run RViz on remote PC You can now fully operate and debug Titan Robot remotely \ud83d\ude80","title":"Networking and Remote Access"},{"location":"networking/#networking-remote-access","text":"","title":"Networking &amp; Remote Access"},{"location":"networking/#overview","text":"Networking and remote access allow you to control, monitor, and debug Titan Robot without directly connecting a keyboard and monitor. This section explains how to connect the robot to a network, access it remotely, and configure ROS 2 communication across machines. Titan Robot supports: - WiFi and Ethernet networking - SSH-based remote access - Remote RViz visualization - Multi-machine ROS 2 setups","title":"Overview"},{"location":"networking/#network-requirements","text":"Robot and control PC must be on the same network Stable WiFi connection recommended for visualization and debugging","title":"Network Requirements"},{"location":"networking/#connecting-the-robot-to-wifi","text":"","title":"Connecting the Robot to WiFi"},{"location":"networking/#using-network-manager-recommended","text":"List available WiFi networks: nmcli device wifi list Connect to a WiFi network: nmcli device wifi connect \"YOUR_SSID\" password \"YOUR_PASSWORD\" Verify connection: ip addr Look for a valid IP address assigned to the wireless interface.","title":"Using Network Manager (Recommended)"},{"location":"networking/#finding-the-robots-ip-address","text":"On the robot, run: hostname -I This will display the robot\u2019s IP address.","title":"Finding the Robot\u2019s IP Address"},{"location":"networking/#finding-the-robot-from-another-device","text":"If you do not know the IP address, scan the network: sudo apt install nmap sudo nmap -sn 192.168.1.0/24 Look for the device corresponding to the robot.","title":"Finding the Robot from Another Device"},{"location":"networking/#ssh-remote-access","text":"SSH allows you to access the robot\u2019s terminal remotely.","title":"SSH Remote Access"},{"location":"networking/#enable-ssh-if-not-already-enabled","text":"sudo systemctl enable ssh sudo systemctl start ssh","title":"Enable SSH (if not already enabled)"},{"location":"networking/#connecting-via-ssh","text":"From your laptop or desktop: ssh <username>@<robot_ip_address> Example: ssh titan@192.168.1.42 Once connected, you can run ROS 2 commands as if you were physically on the robot.","title":"Connecting via SSH"},{"location":"networking/#sourcing-ros-2","text":"After logging in via SSH, ensure ROS 2 is sourced: source /opt/ros/humble/setup.bash source ~/titan_ws/install/setup.bash","title":"Sourcing ROS 2"},{"location":"networking/#remote-rviz2-usage","text":"RViz2 can be run either: - On the robot - On a remote PC connected over the network Running RViz2 remotely is recommended to reduce CPU load on the robot.","title":"Remote RViz2 Usage"},{"location":"networking/#running-rviz2-on-a-remote-pc","text":"Ensure: - ROS 2 Humble is installed on the remote PC - The same workspace (or compatible message definitions) is available Set the same ROS 2 domain ID on both machines.","title":"Running RViz2 on a Remote PC"},{"location":"networking/#ros-2-networking-configuration","text":"ROS 2 uses DDS for communication and works automatically when machines are on the same network.","title":"ROS 2 Networking Configuration"},{"location":"networking/#ros_domain_id","text":"Set the same domain ID on all machines: export ROS_DOMAIN_ID=0 Add this to .bashrc on both robot and remote PC to persist.","title":"ROS_DOMAIN_ID"},{"location":"networking/#verifying-ros-2-communication","text":"On the remote PC: ros2 node list ros2 topic list If nodes and topics from the robot appear, communication is working correctly.","title":"Verifying ROS 2 Communication"},{"location":"networking/#multi-machine-ros-2-setup","text":"Common setup: - Robot runs sensors, control, and navigation - Remote PC runs RViz2 and debugging tools Ensure: - Same ROS_DOMAIN_ID - Same ROS 2 distribution - Firewall does not block DDS traffic","title":"Multi-Machine ROS 2 Setup"},{"location":"networking/#firewall-considerations","text":"If communication issues occur, check firewall settings: sudo ufw status If enabled, allow traffic or temporarily disable for testing: sudo ufw disable","title":"Firewall Considerations"},{"location":"networking/#ethernet-connection-optional","text":"For development and testing: - Connect robot directly to a PC using Ethernet - Assign IP addresses automatically or manually - Provides lower latency and higher reliability than WiFi","title":"Ethernet Connection (Optional)"},{"location":"networking/#security-best-practices","text":"Change default passwords Disable unused services Use SSH keys instead of passwords Avoid running robot on public networks","title":"Security Best Practices"},{"location":"networking/#common-networking-issues","text":"Issue Possible Cause Solution Cannot SSH Wrong IP or SSH disabled Verify IP and SSH service ROS topics not visible Different ROS_DOMAIN_ID Set same domain ID High latency Weak WiFi signal Move closer or use Ethernet RViz lag Running on robot Run RViz on remote PC You can now fully operate and debug Titan Robot remotely \ud83d\ude80","title":"Common Networking Issues"},{"location":"simulation/","text":"Simulation Overview Simulation allows you to test Titan Robot\u2019s software stack in a virtual environment before running it on real hardware. Titan Robot uses Ignition Gazebo (Gazebo Fortress) integrated with ROS 2 through ros_gz_sim to provide a realistic simulation of sensors, robot motion, mapping, and navigation. Simulation is strongly recommended for: - Testing navigation parameters - Debugging SLAM and localization - Developing new features safely - Learning ROS 2 workflows without hardware risk Simulation Stack The simulation setup consists of: Ignition Gazebo (Gazebo Fortress) ros_gz_sim for ROS 2 \u2194 Gazebo integration Simulated Titan Robot model Simulated sensors (LIDAR) RViz2 for visualization Setup Installing Ignition Gazebo (Gazebo Fortress) Install Ignition Gazebo Fortress: sudo apt update sudo apt install ignition-fortress Verify installation: ign gazebo --version Installing ROS\u2013Gazebo Integration Install the ROS 2 Gazebo integration packages: sudo apt install ros-humble-ros-gz-sim sudo apt install ros-humble-ros-gz-bridge sudo apt install ros-humble-ros-gz-image These packages enable communication between ROS 2 and Ignition Gazebo. Preparing the Workspace Ensure your Titan Robot workspace is built and sourced: cd ~/titan_ws colcon build source install/setup.bash Launching the Simulation Start the simulation using the Titan Robot simulation bringup: ros2 launch titan_nav bringup.launch.py This launch file performs the following actions: Starts Ignition Gazebo Loads the warehouse world Spawns the Titan Robot model Launches simulated LIDAR Bridges Gazebo sensor data to ROS 2 Starts RViz2 with a preconfigured view Verifying the Simulation Once launched, verify the following: The robot appears in the Gazebo warehouse world RViz2 shows the robot model LIDAR scan data is visible in RViz2 TF frames are correctly published At this stage, the robot is ready for mapping. Mapping in Simulation Launching SLAM Open a new terminal, source the workspace, and launch SLAM: source ~/titan_ws/install/setup.bash ros2 launch titan_nav slam.launch.py This launch file runs SLAM Toolbox in online asynchronous mode , using simulated LIDAR data. Visualizing the Map In RViz2: Set Fixed Frame to map Add the Map display if not already visible Observe the map being generated as the robot moves You can use teleoperation to explore the environment and build the map. Saving the Map Once mapping is complete: In RViz2, open the Slam Toolbox Plugin Click Save Map Choose a directory and map name This generates a .yaml and .pgm map file that can be reused for navigation. Navigation in Simulation After saving the map, stop the SLAM launch and start navigation. Run the following command in a new terminal: ros2 launch titan_nav nav2.launch.py use_sim_time:=true map:=/absolute/path/to/your/map.yaml This launch file starts: Map server Localization Navigation stack (Nav2) Costmaps and planners The use_sim_time argument ensures synchronization with simulation time. Sending Navigation Goals In RViz2: Set Fixed Frame to map and give 2D Pose Estimate Use the 2D Goal Pose tool Select a goal location in the map The robot will autonomously navigate to the goal within the simulated environment. Simulation Time Simulation runs on a simulated clock. Ensure that: - use_sim_time is set to true - Nodes are synchronized to Gazebo time This is critical for correct SLAM and navigation behavior. Common Simulation Issues Issue Possible Cause Solution Robot not moving Controllers not loaded Check bringup launch logs No LIDAR data Bridge not running Verify ros_gz bridges Navigation fails Map not loaded Check map file path Time-related errors use_sim_time false Enable simulation time Simulation vs Real Robot Key differences to keep in mind: Simulation has perfect sensor alignment No wheel slip unless modeled Odometry is ideal compared to real robot Navigation may appear smoother in simulation","title":"Simulation"},{"location":"simulation/#simulation","text":"","title":"Simulation"},{"location":"simulation/#overview","text":"Simulation allows you to test Titan Robot\u2019s software stack in a virtual environment before running it on real hardware. Titan Robot uses Ignition Gazebo (Gazebo Fortress) integrated with ROS 2 through ros_gz_sim to provide a realistic simulation of sensors, robot motion, mapping, and navigation. Simulation is strongly recommended for: - Testing navigation parameters - Debugging SLAM and localization - Developing new features safely - Learning ROS 2 workflows without hardware risk","title":"Overview"},{"location":"simulation/#simulation-stack","text":"The simulation setup consists of: Ignition Gazebo (Gazebo Fortress) ros_gz_sim for ROS 2 \u2194 Gazebo integration Simulated Titan Robot model Simulated sensors (LIDAR) RViz2 for visualization","title":"Simulation Stack"},{"location":"simulation/#setup","text":"","title":"Setup"},{"location":"simulation/#installing-ignition-gazebo-gazebo-fortress","text":"Install Ignition Gazebo Fortress: sudo apt update sudo apt install ignition-fortress Verify installation: ign gazebo --version","title":"Installing Ignition Gazebo (Gazebo Fortress)"},{"location":"simulation/#installing-rosgazebo-integration","text":"Install the ROS 2 Gazebo integration packages: sudo apt install ros-humble-ros-gz-sim sudo apt install ros-humble-ros-gz-bridge sudo apt install ros-humble-ros-gz-image These packages enable communication between ROS 2 and Ignition Gazebo.","title":"Installing ROS\u2013Gazebo Integration"},{"location":"simulation/#preparing-the-workspace","text":"Ensure your Titan Robot workspace is built and sourced: cd ~/titan_ws colcon build source install/setup.bash","title":"Preparing the Workspace"},{"location":"simulation/#launching-the-simulation","text":"Start the simulation using the Titan Robot simulation bringup: ros2 launch titan_nav bringup.launch.py This launch file performs the following actions: Starts Ignition Gazebo Loads the warehouse world Spawns the Titan Robot model Launches simulated LIDAR Bridges Gazebo sensor data to ROS 2 Starts RViz2 with a preconfigured view","title":"Launching the Simulation"},{"location":"simulation/#verifying-the-simulation","text":"Once launched, verify the following: The robot appears in the Gazebo warehouse world RViz2 shows the robot model LIDAR scan data is visible in RViz2 TF frames are correctly published At this stage, the robot is ready for mapping.","title":"Verifying the Simulation"},{"location":"simulation/#mapping-in-simulation","text":"","title":"Mapping in Simulation"},{"location":"simulation/#launching-slam","text":"Open a new terminal, source the workspace, and launch SLAM: source ~/titan_ws/install/setup.bash ros2 launch titan_nav slam.launch.py This launch file runs SLAM Toolbox in online asynchronous mode , using simulated LIDAR data.","title":"Launching SLAM"},{"location":"simulation/#visualizing-the-map","text":"In RViz2: Set Fixed Frame to map Add the Map display if not already visible Observe the map being generated as the robot moves You can use teleoperation to explore the environment and build the map.","title":"Visualizing the Map"},{"location":"simulation/#saving-the-map","text":"Once mapping is complete: In RViz2, open the Slam Toolbox Plugin Click Save Map Choose a directory and map name This generates a .yaml and .pgm map file that can be reused for navigation.","title":"Saving the Map"},{"location":"simulation/#navigation-in-simulation","text":"After saving the map, stop the SLAM launch and start navigation. Run the following command in a new terminal: ros2 launch titan_nav nav2.launch.py use_sim_time:=true map:=/absolute/path/to/your/map.yaml This launch file starts: Map server Localization Navigation stack (Nav2) Costmaps and planners The use_sim_time argument ensures synchronization with simulation time.","title":"Navigation in Simulation"},{"location":"simulation/#sending-navigation-goals","text":"In RViz2: Set Fixed Frame to map and give 2D Pose Estimate Use the 2D Goal Pose tool Select a goal location in the map The robot will autonomously navigate to the goal within the simulated environment.","title":"Sending Navigation Goals"},{"location":"simulation/#simulation-time","text":"Simulation runs on a simulated clock. Ensure that: - use_sim_time is set to true - Nodes are synchronized to Gazebo time This is critical for correct SLAM and navigation behavior.","title":"Simulation Time"},{"location":"simulation/#common-simulation-issues","text":"Issue Possible Cause Solution Robot not moving Controllers not loaded Check bringup launch logs No LIDAR data Bridge not running Verify ros_gz bridges Navigation fails Map not loaded Check map file path Time-related errors use_sim_time false Enable simulation time","title":"Common Simulation Issues"},{"location":"simulation/#simulation-vs-real-robot","text":"Key differences to keep in mind: Simulation has perfect sensor alignment No wheel slip unless modeled Odometry is ideal compared to real robot Navigation may appear smoother in simulation","title":"Simulation vs Real Robot"},{"location":"software/","text":"Software Architecture Overview This section describes the complete software architecture of Titan Robot , including the ROS 2 system running on the main computer and the firmware running on the microcontroller. The software is designed using a layered approach that cleanly separates high-level autonomy from low-level real-time control. Titan Robot uses ROS 2 Humble for all high-level robot functionality and an ESP32 microcontroller programmed using the Arduino IDE for motor control and odometry computation. High-Level Architecture The software stack is divided into two main layers: High-Level ROS 2 Layer (Raspberry Pi) Low-Level Firmware Layer (ESP32) These layers communicate through a serial interface. High-Level ROS 2 Layer (Raspberry Pi) The main computer runs Ubuntu 22.04 with ROS 2 Humble and is responsible for: Robot bringup and system coordination Mapping and localization Autonomous navigation Sensor processing Visualization and user interaction Core ROS 2 Components Robot Bringup Launch files initialize: Robot description (URDF) Sensor drivers ESP32 communication node RViz2 configuration Navigation Stack Uses ROS 2 Navigation (Nav2) Consumes: Odometry data Laser scan data Produces: Velocity commands ( /cmd_vel ) SLAM and Localization SLAM Toolbox used for: Online mapping Localization using pre-built maps Uses laser scans and odometry as inputs Perception (Variant Dependent) V2 : Image pipeline from Pi Camera V3 : RGB-D and depth processing from RealSense camera Used for object detection and advanced perception tasks ROS 2 Communication Interfaces Key Topics Topic Direction Description /cmd_vel Subscribed Velocity commands from navigation or teleop /odom Published Robot odometry from encoder data /scan Published 2D LIDAR scan data /tf Published Coordinate frame transforms /camera/image Published Camera image stream (V2 / V3) Low-Level Firmware Layer (ESP32) Firmware Overview The ESP32 runs custom firmware developed and flashed using the Arduino IDE . Its primary responsibility is real-time motor control and odometry estimation. The firmware operates independently from ROS 2 timing constraints, ensuring stable and deterministic motor behavior. Firmware Responsibilities The ESP32 firmware performs the following tasks: Reads wheel encoder tick counts Computes wheel velocities Applies PID control to motors Computes robot odometry Exchanges data with the Raspberry Pi via serial Odometry Computation Wheel encoders generate tick counts Tick counts are converted to wheel rotation Wheel rotation is used to calculate: Linear displacement Angular displacement Robot pose ( x , y , \u03b8 ) is estimated using differential-drive kinematics Odometry data is sent over serial to the Raspberry Pi Velocity Command Flow Navigation stack or teleop publishes /cmd_vel Raspberry Pi serial node sends velocity commands to ESP32 ESP32: Converts velocity commands to wheel targets Runs PID control loop Updates motor PWM outputs Serial Communication Bidirectional serial communication between ESP32 and Raspberry Pi Data exchanged: Incoming: linear and angular velocity commands Outgoing: odometry and status information Simple, structured protocol for reliability ROS 2 Odometry Publishing On the Raspberry Pi: Serial data is received from ESP32 Encoder-based odometry is decoded Odometry is published as: /odom topic Corresponding TF transforms ( odom \u2192 base_link ) This odometry is then used by: - SLAM Toolbox - Navigation Stack - Visualization tools Timing and Synchronization ESP32 runs control loops at a fixed rate Raspberry Pi handles ROS 2 timing Time stamps are applied on the ROS 2 side Sensor fusion compensates for accumulated odometry error You now have a complete understanding of Titan Robot\u2019s software architecture \ud83d\ude80","title":"Software Architecture"},{"location":"software/#software-architecture","text":"","title":"Software Architecture"},{"location":"software/#overview","text":"This section describes the complete software architecture of Titan Robot , including the ROS 2 system running on the main computer and the firmware running on the microcontroller. The software is designed using a layered approach that cleanly separates high-level autonomy from low-level real-time control. Titan Robot uses ROS 2 Humble for all high-level robot functionality and an ESP32 microcontroller programmed using the Arduino IDE for motor control and odometry computation.","title":"Overview"},{"location":"software/#high-level-architecture","text":"The software stack is divided into two main layers: High-Level ROS 2 Layer (Raspberry Pi) Low-Level Firmware Layer (ESP32) These layers communicate through a serial interface.","title":"High-Level Architecture"},{"location":"software/#high-level-ros-2-layer-raspberry-pi","text":"The main computer runs Ubuntu 22.04 with ROS 2 Humble and is responsible for: Robot bringup and system coordination Mapping and localization Autonomous navigation Sensor processing Visualization and user interaction","title":"High-Level ROS 2 Layer (Raspberry Pi)"},{"location":"software/#core-ros-2-components","text":"","title":"Core ROS 2 Components"},{"location":"software/#robot-bringup","text":"Launch files initialize: Robot description (URDF) Sensor drivers ESP32 communication node RViz2 configuration","title":"Robot Bringup"},{"location":"software/#navigation-stack","text":"Uses ROS 2 Navigation (Nav2) Consumes: Odometry data Laser scan data Produces: Velocity commands ( /cmd_vel )","title":"Navigation Stack"},{"location":"software/#slam-and-localization","text":"SLAM Toolbox used for: Online mapping Localization using pre-built maps Uses laser scans and odometry as inputs","title":"SLAM and Localization"},{"location":"software/#perception-variant-dependent","text":"V2 : Image pipeline from Pi Camera V3 : RGB-D and depth processing from RealSense camera Used for object detection and advanced perception tasks","title":"Perception (Variant Dependent)"},{"location":"software/#ros-2-communication-interfaces","text":"","title":"ROS 2 Communication Interfaces"},{"location":"software/#key-topics","text":"Topic Direction Description /cmd_vel Subscribed Velocity commands from navigation or teleop /odom Published Robot odometry from encoder data /scan Published 2D LIDAR scan data /tf Published Coordinate frame transforms /camera/image Published Camera image stream (V2 / V3)","title":"Key Topics"},{"location":"software/#low-level-firmware-layer-esp32","text":"","title":"Low-Level Firmware Layer (ESP32)"},{"location":"software/#firmware-overview","text":"The ESP32 runs custom firmware developed and flashed using the Arduino IDE . Its primary responsibility is real-time motor control and odometry estimation. The firmware operates independently from ROS 2 timing constraints, ensuring stable and deterministic motor behavior.","title":"Firmware Overview"},{"location":"software/#firmware-responsibilities","text":"The ESP32 firmware performs the following tasks: Reads wheel encoder tick counts Computes wheel velocities Applies PID control to motors Computes robot odometry Exchanges data with the Raspberry Pi via serial","title":"Firmware Responsibilities"},{"location":"software/#odometry-computation","text":"Wheel encoders generate tick counts Tick counts are converted to wheel rotation Wheel rotation is used to calculate: Linear displacement Angular displacement Robot pose ( x , y , \u03b8 ) is estimated using differential-drive kinematics Odometry data is sent over serial to the Raspberry Pi","title":"Odometry Computation"},{"location":"software/#velocity-command-flow","text":"Navigation stack or teleop publishes /cmd_vel Raspberry Pi serial node sends velocity commands to ESP32 ESP32: Converts velocity commands to wheel targets Runs PID control loop Updates motor PWM outputs","title":"Velocity Command Flow"},{"location":"software/#serial-communication","text":"Bidirectional serial communication between ESP32 and Raspberry Pi Data exchanged: Incoming: linear and angular velocity commands Outgoing: odometry and status information Simple, structured protocol for reliability","title":"Serial Communication"},{"location":"software/#ros-2-odometry-publishing","text":"On the Raspberry Pi: Serial data is received from ESP32 Encoder-based odometry is decoded Odometry is published as: /odom topic Corresponding TF transforms ( odom \u2192 base_link ) This odometry is then used by: - SLAM Toolbox - Navigation Stack - Visualization tools","title":"ROS 2 Odometry Publishing"},{"location":"software/#timing-and-synchronization","text":"ESP32 runs control loops at a fixed rate Raspberry Pi handles ROS 2 timing Time stamps are applied on the ROS 2 side Sensor fusion compensates for accumulated odometry error You now have a complete understanding of Titan Robot\u2019s software architecture \ud83d\ude80","title":"Timing and Synchronization"},{"location":"troubleshoot/","text":"Troubleshooting & FAQ Overview This section provides guidance for diagnosing and resolving common issues encountered while using Titan Robot. It is intended as a quick reference during development, testing, and deployment. The Troubleshooting section focuses on symptom-based diagnosis, while the FAQ addresses common conceptual and usage questions. Troubleshooting Robot Bringup Issues Symptom Possible Cause Recommended Action Bringup launch fails Workspace not built Run colcon build and source the workspace Nodes not starting Missing dependencies Install required ROS 2 packages RViz2 does not open Display issue or misconfiguration Launch RViz manually and check logs Robot description not visible URDF not loaded Verify robot_state_publisher is running Motion & Motor Issues Symptom Possible Cause Recommended Action Robot does not move No velocity commands Check /cmd_vel topic Wheels move unevenly PID not tuned Tune motor PID parameters Robot jerks or oscillates Aggressive control gains Reduce PID gains Motors unresponsive Power or wiring issue Verify motor power and connections Odometry & Localization Issues Symptom Possible Cause Recommended Action Odometry not updating Serial communication failure Check ESP32 connection Robot pose jumps Incorrect initial pose Reset pose in RViz Drift at standstill Encoder noise Improve encoder filtering Robot rotates in place Incorrect wheel parameters Verify wheel separation and radius LIDAR & Sensor Issues Symptom Possible Cause Recommended Action No LIDAR data USB connection issue Reconnect LIDAR and restart driver Laser scan rotated Incorrect TF Verify LIDAR frame alignment Incomplete scan Sensor obstruction Remove physical obstructions No camera feed Camera not detected Check camera driver and permissions Navigation Issues (Nav2) Symptom Possible Cause Recommended Action No path generated Localization not converged Set initial pose again Robot stuck Obstacle inflation too large Reduce inflation radius Collisions Incorrect footprint Update robot footprint Oscillations Controller tuning issue Tune Nav2 controller parameters Networking & Remote Access Issues Symptom Possible Cause Recommended Action Cannot SSH SSH service disabled Enable and start SSH ROS topics not visible Different ROS_DOMAIN_ID Set same domain ID RViz lag Running RViz on robot Run RViz remotely High latency Weak WiFi signal Use Ethernet or improve WiFi General Debugging Checklist Step Action 1 Check power and battery level 2 Verify robot bringup launched correctly 3 Confirm /scan and /odom are publishing 4 Check TF tree consistency 5 Verify Nav2 nodes are active 6 Test teleoperation Frequently Asked Questions (FAQ) Q: The robot is powered on but not responding. What should I check first? A: Verify battery voltage, motor power, and ensure the ESP32 firmware is running and connected. Q: Why does the robot drift even when standing still? A: This is usually caused by encoder noise or incorrect PID tuning. Check encoder filtering and reduce control gains. Q: Do I need to remap the environment every time? A: No. As long as the environment and sensor mounting remain unchanged, a saved map can be reused. Q: Can I run RViz2 on my laptop instead of the robot? A: Yes. Running RViz2 remotely is recommended to reduce CPU load on the robot. Q: Why does Nav2 fail to generate a path? A: Common causes include poor localization, incorrect costmap configuration, or an invalid initial pose. Q: Is it safe to run navigation without teleoperation testing? A: No. Always verify basic teleoperation and sensor data before enabling autonomous navigation. Q: Can I add new sensors to the robot? A: Yes. Titan Robot is designed to be modular and supports additional sensors via USB or GPIO. Q: How do I stop the robot immediately in case of an issue? A: Use keyboard teleop to stop motion, power off the motors, or disconnect power if necessary. When to Seek Help If an issue persists after troubleshooting: Check logs using rqt_console Verify configuration files Review recent changes Consult project documentation or community channels This concludes the core documentation for Titan Robot \ud83d\ude80","title":"Troubleshooting"},{"location":"troubleshoot/#troubleshooting-faq","text":"","title":"Troubleshooting &amp; FAQ"},{"location":"troubleshoot/#overview","text":"This section provides guidance for diagnosing and resolving common issues encountered while using Titan Robot. It is intended as a quick reference during development, testing, and deployment. The Troubleshooting section focuses on symptom-based diagnosis, while the FAQ addresses common conceptual and usage questions.","title":"Overview"},{"location":"troubleshoot/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"troubleshoot/#robot-bringup-issues","text":"Symptom Possible Cause Recommended Action Bringup launch fails Workspace not built Run colcon build and source the workspace Nodes not starting Missing dependencies Install required ROS 2 packages RViz2 does not open Display issue or misconfiguration Launch RViz manually and check logs Robot description not visible URDF not loaded Verify robot_state_publisher is running","title":"Robot Bringup Issues"},{"location":"troubleshoot/#motion-motor-issues","text":"Symptom Possible Cause Recommended Action Robot does not move No velocity commands Check /cmd_vel topic Wheels move unevenly PID not tuned Tune motor PID parameters Robot jerks or oscillates Aggressive control gains Reduce PID gains Motors unresponsive Power or wiring issue Verify motor power and connections","title":"Motion &amp; Motor Issues"},{"location":"troubleshoot/#odometry-localization-issues","text":"Symptom Possible Cause Recommended Action Odometry not updating Serial communication failure Check ESP32 connection Robot pose jumps Incorrect initial pose Reset pose in RViz Drift at standstill Encoder noise Improve encoder filtering Robot rotates in place Incorrect wheel parameters Verify wheel separation and radius","title":"Odometry &amp; Localization Issues"},{"location":"troubleshoot/#lidar-sensor-issues","text":"Symptom Possible Cause Recommended Action No LIDAR data USB connection issue Reconnect LIDAR and restart driver Laser scan rotated Incorrect TF Verify LIDAR frame alignment Incomplete scan Sensor obstruction Remove physical obstructions No camera feed Camera not detected Check camera driver and permissions","title":"LIDAR &amp; Sensor Issues"},{"location":"troubleshoot/#navigation-issues-nav2","text":"Symptom Possible Cause Recommended Action No path generated Localization not converged Set initial pose again Robot stuck Obstacle inflation too large Reduce inflation radius Collisions Incorrect footprint Update robot footprint Oscillations Controller tuning issue Tune Nav2 controller parameters","title":"Navigation Issues (Nav2)"},{"location":"troubleshoot/#networking-remote-access-issues","text":"Symptom Possible Cause Recommended Action Cannot SSH SSH service disabled Enable and start SSH ROS topics not visible Different ROS_DOMAIN_ID Set same domain ID RViz lag Running RViz on robot Run RViz remotely High latency Weak WiFi signal Use Ethernet or improve WiFi","title":"Networking &amp; Remote Access Issues"},{"location":"troubleshoot/#general-debugging-checklist","text":"Step Action 1 Check power and battery level 2 Verify robot bringup launched correctly 3 Confirm /scan and /odom are publishing 4 Check TF tree consistency 5 Verify Nav2 nodes are active 6 Test teleoperation","title":"General Debugging Checklist"},{"location":"troubleshoot/#frequently-asked-questions-faq","text":"","title":"Frequently Asked Questions (FAQ)"},{"location":"troubleshoot/#q-the-robot-is-powered-on-but-not-responding-what-should-i-check-first","text":"A: Verify battery voltage, motor power, and ensure the ESP32 firmware is running and connected.","title":"Q: The robot is powered on but not responding. What should I check first?"},{"location":"troubleshoot/#q-why-does-the-robot-drift-even-when-standing-still","text":"A: This is usually caused by encoder noise or incorrect PID tuning. Check encoder filtering and reduce control gains.","title":"Q: Why does the robot drift even when standing still?"},{"location":"troubleshoot/#q-do-i-need-to-remap-the-environment-every-time","text":"A: No. As long as the environment and sensor mounting remain unchanged, a saved map can be reused.","title":"Q: Do I need to remap the environment every time?"},{"location":"troubleshoot/#q-can-i-run-rviz2-on-my-laptop-instead-of-the-robot","text":"A: Yes. Running RViz2 remotely is recommended to reduce CPU load on the robot.","title":"Q: Can I run RViz2 on my laptop instead of the robot?"},{"location":"troubleshoot/#q-why-does-nav2-fail-to-generate-a-path","text":"A: Common causes include poor localization, incorrect costmap configuration, or an invalid initial pose.","title":"Q: Why does Nav2 fail to generate a path?"},{"location":"troubleshoot/#q-is-it-safe-to-run-navigation-without-teleoperation-testing","text":"A: No. Always verify basic teleoperation and sensor data before enabling autonomous navigation.","title":"Q: Is it safe to run navigation without teleoperation testing?"},{"location":"troubleshoot/#q-can-i-add-new-sensors-to-the-robot","text":"A: Yes. Titan Robot is designed to be modular and supports additional sensors via USB or GPIO.","title":"Q: Can I add new sensors to the robot?"},{"location":"troubleshoot/#q-how-do-i-stop-the-robot-immediately-in-case-of-an-issue","text":"A: Use keyboard teleop to stop motion, power off the motors, or disconnect power if necessary.","title":"Q: How do I stop the robot immediately in case of an issue?"},{"location":"troubleshoot/#when-to-seek-help","text":"If an issue persists after troubleshooting: Check logs using rqt_console Verify configuration files Review recent changes Consult project documentation or community channels This concludes the core documentation for Titan Robot \ud83d\ude80","title":"When to Seek Help"},{"location":"visualization/","text":"Visualization & Debugging Overview Visualization and debugging tools are essential for understanding robot behavior, validating sensor data, and diagnosing issues during development. This section introduces the primary tools used with Titan Robot to visualize data and debug the ROS 2 system. The main tools covered are: RViz2 rqt tools ROS 2 command-line utilities RViz2 Overview RViz2 is the primary visualization tool used to view the robot\u2019s state, sensor data, and navigation information. With RViz2, you can visualize: Robot model (URDF) Coordinate frames (TF) Laser scan data Odometry Maps Navigation goals and paths RViz2 is typically launched automatically during robot bringup. RViz2 Configuration Fixed Frame Ensure the Fixed Frame is set correctly: Recommended: odom (or map when mapping/localization is active) Incorrect fixed frames are a common source of visualization issues. Common RViz Displays Recommended displays to enable: RobotModel \u2013 Shows the robot URDF TF \u2013 Visualizes coordinate frames LaserScan \u2013 Displays LIDAR data Odometry \u2013 Visualizes robot motion Map \u2013 Shows generated or loaded maps Path \u2013 Displays navigation paths Verifying Sensor Data LIDAR Visualization Add a LaserScan display Topic: /scan Verify that obstacles appear correctly around the robot Ensure scan rotates with the robot frame Camera Visualization (V3) Add an Image display Topic: V2: /camera/image_raw V3: /camera/color/image_raw Confirm image stream updates in real time TF Debugging TF (transform) errors are one of the most common issues in ROS-based systems. Visualizing TF Frames Enable the TF display in RViz2 to: Verify correct frame hierarchy Check transform directions Identify missing or misaligned frames Common frames include: map odom base_link laser camera_link Generating TF Tree ros2 run tf2_tools view_frames This generates a PDF showing the complete TF tree. Odometry Debugging Checking Odometry Topic ros2 topic echo /odom Verify that: Position updates when the robot moves Orientation changes during rotation No sudden jumps or drift at standstill Visualizing Odometry Add Odometry display in RViz2 Check smooth motion and correct orientation Verify odom frame alignment rqt Tools rqt provides GUI-based debugging and introspection tools. rqt_graph rqt_graph Visualizes node and topic connections Useful for verifying data flow Helps detect missing or misconnected nodes rqt_plot rqt_plot Plot values over time Useful for: Odometry velocity Encoder-derived data Sensor readings rqt_console rqt_console Displays ROS 2 log messages Filter logs by severity and node Useful for runtime error detection Command-Line Debugging Tools List Nodes ros2 node list Inspect Node Info ros2 node info <node_name> Topic Frequency ros2 topic hz /scan ros2 topic hz /odom Ensures data is being published at expected rates. Common Debugging Scenarios Issue Debugging Step Robot not moving Check /cmd_vel and serial node No LIDAR data Check /scan topic and USB connection Navigation fails Inspect costmaps and TF Map drifting Check odometry accuracy RViz empty Verify fixed frame and TF tree Debugging Best Practices Always check /tf first Verify sensor topics before navigation Use low speeds during testing Debug one subsystem at a time Keep RViz displays minimal to improve performance With proper visualization and debugging, diagnosing robot issues becomes significantly easier \ud83d\ude80","title":"Visualization"},{"location":"visualization/#visualization-debugging","text":"","title":"Visualization &amp; Debugging"},{"location":"visualization/#overview","text":"Visualization and debugging tools are essential for understanding robot behavior, validating sensor data, and diagnosing issues during development. This section introduces the primary tools used with Titan Robot to visualize data and debug the ROS 2 system. The main tools covered are: RViz2 rqt tools ROS 2 command-line utilities","title":"Overview"},{"location":"visualization/#rviz2-overview","text":"RViz2 is the primary visualization tool used to view the robot\u2019s state, sensor data, and navigation information. With RViz2, you can visualize: Robot model (URDF) Coordinate frames (TF) Laser scan data Odometry Maps Navigation goals and paths RViz2 is typically launched automatically during robot bringup.","title":"RViz2 Overview"},{"location":"visualization/#rviz2-configuration","text":"","title":"RViz2 Configuration"},{"location":"visualization/#fixed-frame","text":"Ensure the Fixed Frame is set correctly: Recommended: odom (or map when mapping/localization is active) Incorrect fixed frames are a common source of visualization issues.","title":"Fixed Frame"},{"location":"visualization/#common-rviz-displays","text":"Recommended displays to enable: RobotModel \u2013 Shows the robot URDF TF \u2013 Visualizes coordinate frames LaserScan \u2013 Displays LIDAR data Odometry \u2013 Visualizes robot motion Map \u2013 Shows generated or loaded maps Path \u2013 Displays navigation paths","title":"Common RViz Displays"},{"location":"visualization/#verifying-sensor-data","text":"","title":"Verifying Sensor Data"},{"location":"visualization/#lidar-visualization","text":"Add a LaserScan display Topic: /scan Verify that obstacles appear correctly around the robot Ensure scan rotates with the robot frame","title":"LIDAR Visualization"},{"location":"visualization/#camera-visualization-v3","text":"Add an Image display Topic: V2: /camera/image_raw V3: /camera/color/image_raw Confirm image stream updates in real time","title":"Camera Visualization (V3)"},{"location":"visualization/#tf-debugging","text":"TF (transform) errors are one of the most common issues in ROS-based systems.","title":"TF Debugging"},{"location":"visualization/#visualizing-tf-frames","text":"Enable the TF display in RViz2 to: Verify correct frame hierarchy Check transform directions Identify missing or misaligned frames Common frames include: map odom base_link laser camera_link","title":"Visualizing TF Frames"},{"location":"visualization/#generating-tf-tree","text":"ros2 run tf2_tools view_frames This generates a PDF showing the complete TF tree.","title":"Generating TF Tree"},{"location":"visualization/#odometry-debugging","text":"","title":"Odometry Debugging"},{"location":"visualization/#checking-odometry-topic","text":"ros2 topic echo /odom Verify that: Position updates when the robot moves Orientation changes during rotation No sudden jumps or drift at standstill","title":"Checking Odometry Topic"},{"location":"visualization/#visualizing-odometry","text":"Add Odometry display in RViz2 Check smooth motion and correct orientation Verify odom frame alignment","title":"Visualizing Odometry"},{"location":"visualization/#rqt-tools","text":"rqt provides GUI-based debugging and introspection tools.","title":"rqt Tools"},{"location":"visualization/#rqt_graph","text":"rqt_graph Visualizes node and topic connections Useful for verifying data flow Helps detect missing or misconnected nodes","title":"rqt_graph"},{"location":"visualization/#rqt_plot","text":"rqt_plot Plot values over time Useful for: Odometry velocity Encoder-derived data Sensor readings","title":"rqt_plot"},{"location":"visualization/#rqt_console","text":"rqt_console Displays ROS 2 log messages Filter logs by severity and node Useful for runtime error detection","title":"rqt_console"},{"location":"visualization/#command-line-debugging-tools","text":"","title":"Command-Line Debugging Tools"},{"location":"visualization/#list-nodes","text":"ros2 node list","title":"List Nodes"},{"location":"visualization/#inspect-node-info","text":"ros2 node info <node_name>","title":"Inspect Node Info"},{"location":"visualization/#topic-frequency","text":"ros2 topic hz /scan ros2 topic hz /odom Ensures data is being published at expected rates.","title":"Topic Frequency"},{"location":"visualization/#common-debugging-scenarios","text":"Issue Debugging Step Robot not moving Check /cmd_vel and serial node No LIDAR data Check /scan topic and USB connection Navigation fails Inspect costmaps and TF Map drifting Check odometry accuracy RViz empty Verify fixed frame and TF tree","title":"Common Debugging Scenarios"},{"location":"visualization/#debugging-best-practices","text":"Always check /tf first Verify sensor topics before navigation Use low speeds during testing Debug one subsystem at a time Keep RViz displays minimal to improve performance With proper visualization and debugging, diagnosing robot issues becomes significantly easier \ud83d\ude80","title":"Debugging Best Practices"}]}